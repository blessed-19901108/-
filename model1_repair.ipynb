{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a5315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------+-----------------+---------------+\n",
      "| Industry                         | Fraud_Count | Non_Fraud_Count | Unknown_Count |\n",
      "+----------------------------------+-------------+-----------------+---------------+\n",
      "| 制造业                           | 3013.0      | 10547           | 2500          |\n",
      "| 信息传输、软件和信息技术服务业   | 673.0       | 1347            | 325           |\n",
      "| 批发和零售业                     | 377.0       | 754             | 162           |\n",
      "| 房地产业                         | 256.0       | 513             | 108           |\n",
      "| 金融业                           | 238.0       | 476             | 110           |\n",
      "| 建筑业                           | 202.0       | 405             | 87            |\n",
      "| 电力、热力、燃气及水生产和供应业 | 148.0       | 520             | 110           |\n",
      "| 交通运输、仓储和邮政业           | 133.0       | 466             | 102           |\n",
      "| 科学研究和技术服务业             | 111.0       | 223             | 59            |\n",
      "| 采矿业                           | 102.0       | 358             | 74            |\n",
      "| 农、林、牧、渔业                 | 95.0        | 191             | 41            |\n",
      "| 水利、环境和公共设施管理业       | 77.0        | 272             | 68            |\n",
      "| 文化、体育和娱乐业               | 76.0        | 268             | 57            |\n",
      "| 综合                             | 39.0        | 79              | 16            |\n",
      "| 教育                             | 20.0        | 40              | 9             |\n",
      "| 住宿和餐饮业                     | 0.0         | 46              | 10            |\n",
      "| 卫生和社会工作                   | 0.0         | 59              | 12            |\n",
      "| 居民服务、修理和其他服务业       | 0.0         | 4               | 1             |\n",
      "| 租赁和商务服务业                 | 0.0         | 248             | 54            |\n",
      "+----------------------------------+-------------+-----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv('output/data_partial_balanced.csv')\n",
    "\n",
    "# Flag为1表示造假，Flag为0表示不造假，Flag为空表示未知\n",
    "fraud_counts = df[df['FLAG'] == 1].groupby('Industry').size().reset_index(name='Fraud_Count')\n",
    "non_fraud_counts = df[df['FLAG'] == 0].groupby('Industry').size().reset_index(name='Non_Fraud_Count')\n",
    "unknown_counts = df[df['FLAG'].isna()].groupby('Industry').size().reset_index(name='Unknown_Count')\n",
    "\n",
    "# 合并三个结果，以行业为基准\n",
    "merged = fraud_counts.merge(non_fraud_counts, on='Industry', how='outer').merge(unknown_counts, on='Industry', how='outer').fillna(0)\n",
    "\n",
    "# 按照造假数量降序排序\n",
    "merged_sorted = merged.sort_values(by='Fraud_Count', ascending=False)\n",
    "\n",
    "# 使用tabulate库以表格形式输出，设置所有内容左对齐\n",
    "print(tabulate(merged_sorted.values, merged_sorted.columns, tablefmt='pretty', colalign=('left',)*len(merged_sorted.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf969edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "缺失值统计情况（只显示有缺失值的列）：\n",
      "      Missing_Count  Missing_Percentage\n",
      "FLAG           3905               14.86\n",
      "\n",
      "数据集总行数： 26281\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('output/data_partial_balanced.csv')\n",
    "\n",
    "# 计算每列的缺失值数量和百分比\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "# 只显示有缺失值的列\n",
    "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"\\n缺失值统计情况（只显示有缺失值的列）：\")\n",
    "print(missing_stats)\n",
    "\n",
    "print(\"\\n数据集总行数：\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74d1e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:07:55,896 - __main__ - INFO - Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 财务造假预测：基于交叉验证的Stacking模型 - 修复版\n",
    "# 分行业逐步运行，降低内存占用，避免数据泄露\n",
    "\n",
    "## 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import gc  # 垃圾回收\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.base import clone\n",
    "import logging\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"financial_fraud_model.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    \"\"\"设置所有随机种子以确保可复现性\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置可视化风格\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# 检查是否有GPU可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# 创建必要的目录\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_industry_list():\n",
    "    \"\"\"\n",
    "    生成可用的行业列表并保存，包含统计信息\n",
    "    \"\"\"\n",
    "    # 加载数据\n",
    "    logger.info(\"Generating industry list...\")\n",
    "    try:\n",
    "        financial_data = pd.read_csv('output/data_partial_balanced.csv')\n",
    "        industry_features_df = pd.read_csv('industry_features_results/all_industries_features.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 获取有监督特征选择的行业列表\n",
    "    supervised_industries = industry_features_df[industry_features_df['Method'] == '有监督']['Industry'].unique()\n",
    "    logger.info(f\"Found {len(supervised_industries)} industries with supervised feature selection\")\n",
    "    \n",
    "    # 分离预测集\n",
    "    train_data = financial_data[financial_data['FLAG'].notna()].copy()\n",
    "    \n",
    "    # 检查各行业的样本数量和造假比例\n",
    "    industry_stats = train_data[train_data['Industry'].isin(supervised_industries)].groupby('Industry')['FLAG'].agg(['count', 'sum'])\n",
    "    industry_stats['fraud_ratio'] = industry_stats['sum'] / industry_stats['count']\n",
    "    industry_stats = industry_stats.sort_values('count', ascending=False)\n",
    "    \n",
    "    # 获取每个行业的特征数量\n",
    "    feature_counts = {}\n",
    "    for industry in supervised_industries:\n",
    "        features = industry_features_df[industry_features_df['Industry'] == industry]['Feature'].tolist()\n",
    "        feature_counts[industry] = len(features)\n",
    "    \n",
    "    # 保存行业列表\n",
    "    industry_list = pd.DataFrame({\n",
    "        'Industry': industry_stats.index,\n",
    "        'Sample_Count': industry_stats['count'],\n",
    "        'Fraud_Count': industry_stats['sum'],\n",
    "        'Fraud_Ratio': industry_stats['fraud_ratio'],\n",
    "        'Feature_Count': [feature_counts.get(ind, 0) for ind in industry_stats.index]\n",
    "    })\n",
    "    \n",
    "    # 添加训练/预测可行性标志\n",
    "    # 行业样本数过少（<30）或者特征数过少（<5）可能不适合训练\n",
    "    industry_list['Trainable'] = (industry_list['Sample_Count'] >= 30) & (industry_list['Feature_Count'] >= 5)\n",
    "    \n",
    "    # 检查极度不平衡（<1%或>99%）的样本\n",
    "    industry_list['Imbalanced'] = (industry_list['Fraud_Ratio'] < 0.01) | (industry_list['Fraud_Ratio'] > 0.99)\n",
    "    \n",
    "    # 保存详细行业列表\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    industry_list.to_csv(f'results/industry_list_{timestamp}.csv', index=False)\n",
    "    \n",
    "    # 创建行业统计可视化\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 样本数量条形图\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_industries_by_count = industry_list.sort_values('Sample_Count', ascending=False).head(15)\n",
    "    sns.barplot(x='Sample_Count', y='Industry', data=top_industries_by_count)\n",
    "    plt.title('Top 15 Industries by Sample Count')\n",
    "    plt.xlabel('Sample Count')\n",
    "    \n",
    "    # 造假率条形图\n",
    "    plt.subplot(2, 2, 2)\n",
    "    top_industries_by_fraud = industry_list.sort_values('Fraud_Ratio', ascending=False).head(15)\n",
    "    sns.barplot(x='Fraud_Ratio', y='Industry', data=top_industries_by_fraud)\n",
    "    plt.title('Top 15 Industries by Fraud Ratio')\n",
    "    plt.xlabel('Fraud Ratio')\n",
    "    \n",
    "    # 特征数量条形图\n",
    "    plt.subplot(2, 2, 3)\n",
    "    top_industries_by_features = industry_list.sort_values('Feature_Count', ascending=False).head(15)\n",
    "    sns.barplot(x='Feature_Count', y='Industry', data=top_industries_by_features)\n",
    "    plt.title('Top 15 Industries by Feature Count')\n",
    "    plt.xlabel('Feature Count')\n",
    "    \n",
    "    # 样本量与造假率散点图\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(\n",
    "        industry_list['Sample_Count'], \n",
    "        industry_list['Fraud_Ratio'], \n",
    "        alpha=0.7,\n",
    "        c=industry_list['Trainable'].map({True: 'green', False: 'red'})\n",
    "    )\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Sample Count (log scale)')\n",
    "    plt.ylabel('Fraud Ratio')\n",
    "    plt.title('Sample Count vs Fraud Ratio')\n",
    "    \n",
    "    # 添加注释，标记出样本量最大的几个行业\n",
    "    for i, row in industry_list.sort_values('Sample_Count', ascending=False).head(5).iterrows():\n",
    "        plt.annotate(\n",
    "            row['Industry'],\n",
    "            (row['Sample_Count'], row['Fraud_Ratio']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "        \n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/industry_statistics_{timestamp}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 输出行业统计信息\n",
    "    logger.info(\"\\nAvailable industries summary:\")\n",
    "    logger.info(f\"Total industries: {len(industry_list)}\")\n",
    "    logger.info(f\"Trainable industries: {industry_list['Trainable'].sum()}\")\n",
    "    logger.info(f\"Imbalanced industries: {industry_list['Imbalanced'].sum()}\")\n",
    "    logger.info(f\"Industry with most samples: {industry_list.loc[industry_list['Sample_Count'].idxmax(), 'Industry']} ({industry_list['Sample_Count'].max()} samples)\")\n",
    "    logger.info(f\"Industry with highest fraud ratio: {industry_list.loc[industry_list['Fraud_Ratio'].idxmax(), 'Industry']} ({industry_list['Fraud_Ratio'].max():.2%})\")\n",
    "    \n",
    "    # 输出前10个推荐行业（样本数足够且相对平衡）\n",
    "    recommended = industry_list[\n",
    "        (industry_list['Trainable'] == True) & \n",
    "        (industry_list['Imbalanced'] == False)\n",
    "    ].sort_values('Sample_Count', ascending=False).head(10)\n",
    "    \n",
    "    logger.info(\"\\nTop 10 recommended industries for modeling:\")\n",
    "    for i, (_, row) in enumerate(recommended.iterrows()):\n",
    "        logger.info(f\"{i+1}. {row['Industry']}: {row['Sample_Count']} samples, {row['Fraud_Ratio']:.2%} fraud ratio, {row['Feature_Count']} features\")\n",
    "    \n",
    "    return industry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32696676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b735f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据验证与清洗函数\n",
    "def validate_and_clean_data(data, industry_name=None):\n",
    "    \"\"\"\n",
    "    验证并清洗数据，处理缺失值、异常值等\n",
    "    \n",
    "    Args:\n",
    "        data: 需要验证的DataFrame\n",
    "        industry_name: 行业名称，用于日志\n",
    "    \n",
    "    Returns:\n",
    "        清洗后的DataFrame\n",
    "    \"\"\"\n",
    "    prefix = f\"[{industry_name}] \" if industry_name else \"\"\n",
    "    initial_rows = len(data)\n",
    "    \n",
    "    # 检查并报告缺失值\n",
    "    missing_stats = data.isnull().sum()\n",
    "    missing_cols = missing_stats[missing_stats > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        logger.info(f\"{prefix}发现缺失值: {missing_cols.to_dict()}\")\n",
    "    \n",
    "    # 移除全部为缺失值的行\n",
    "    data = data.dropna(how='all')\n",
    "    \n",
    "    # 检查数值特征的异常值\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if col == 'FLAG':  # 跳过目标变量\n",
    "            continue\n",
    "            \n",
    "        # 使用IQR方法检测异常值\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "        if len(outliers) > 0:\n",
    "            logger.info(f\"{prefix}特征'{col}'发现{len(outliers)}个异常值\")\n",
    "            \n",
    "            # 将极端异常值替换为边界值\n",
    "            data.loc[data[col] < lower_bound, col] = lower_bound\n",
    "            data.loc[data[col] > upper_bound, col] = upper_bound\n",
    "    \n",
    "    # 移除重复行\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    final_rows = len(data)\n",
    "    if final_rows < initial_rows:\n",
    "        logger.info(f\"{prefix}数据清洗: 初始行数={initial_rows}, 最终行数={final_rows}, 移除={initial_rows-final_rows}行\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6e2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4671062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_industry(industry, industry_data, industry_features, test_size=0.2, \n",
    "                           time_series=False, date_col=None, \n",
    "                           use_simplified_models=False, skip_tabnet=False, n_folds=5,\n",
    "                           target_accuracy=0.89, accuracy_margin=0.01):\n",
    "    \"\"\"\n",
    "    处理单个行业的数据，避免数据泄露，根据样本量调整模型复杂度\n",
    "    增加了目标准确率早停机制，在达到89%左右准确率时停止训练\n",
    "    \n",
    "    Args:\n",
    "        industry: 行业名称\n",
    "        industry_data: 该行业的数据\n",
    "        industry_features: 该行业的特征列表\n",
    "        test_size: 测试集比例\n",
    "        time_series: 是否按时间序列处理\n",
    "        date_col: 日期列名称，仅在time_series=True时使用\n",
    "        use_simplified_models: 是否使用简化模型（样本数量少时）\n",
    "        skip_tabnet: 是否跳过TabNet模型（样本数量少时）\n",
    "        n_folds: 交叉验证折数\n",
    "        target_accuracy: 目标准确率，达到此准确率附近时停止训练\n",
    "        accuracy_margin: 准确率容差范围\n",
    "        \n",
    "    Returns:\n",
    "        模型结果\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n\\n{'='*80}\")\n",
    "    logger.info(f\"Processing industry: {industry}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    logger.info(f\"Data shape: {industry_data.shape}\")\n",
    "    logger.info(f\"Number of features: {len(industry_features)}\")\n",
    "    logger.info(f\"Target accuracy: {target_accuracy:.4f} (±{accuracy_margin:.4f})\")\n",
    "    \n",
    "    # 数据验证和清洗\n",
    "    industry_data = validate_and_clean_data(industry_data, industry)\n",
    "    \n",
    "    # 确保所有特征都在数据集中\n",
    "    valid_features = [f for f in industry_features if f in industry_data.columns]\n",
    "    if len(valid_features) < len(industry_features):\n",
    "        logger.warning(f\"Warning: {len(industry_features) - len(valid_features)} features not found for industry {industry}\")\n",
    "        logger.info(f\"Using {len(valid_features)} valid features\")\n",
    "    \n",
    "    \n",
    "    # 检查并处理特征中的 NaN 值\n",
    "    for feature in valid_features:\n",
    "        if industry_data[feature].isna().any():\n",
    "            logger.warning(f\"Feature '{feature}' contains {industry_data[feature].isna().sum()} NaN values. Filling with mean.\")\n",
    "            industry_data[feature] = industry_data[feature].fillna(industry_data[feature].mean())\n",
    "\n",
    "    \n",
    "    # 分离特征和标签\n",
    "    X = industry_data[valid_features]\n",
    "    y = industry_data['FLAG']\n",
    "    \n",
    "    # 检查样本量，确定是否需要调整模型复杂度\n",
    "    sample_size = len(industry_data)\n",
    "    if sample_size < 50:\n",
    "        logger.warning(f\"Very small sample size ({sample_size}) for industry {industry}. Results may be unreliable.\")\n",
    "        logger.warning(\"Using simplified models and increasing regularization.\")\n",
    "        use_simplified_models = True\n",
    "    elif sample_size < 100:\n",
    "        logger.warning(f\"Small sample size ({sample_size}) for industry {industry}. Consider collecting more data.\")\n",
    "        logger.warning(\"Using moderately complex models.\")\n",
    "        use_simplified_models = True\n",
    "    else:\n",
    "        use_simplified_models = False\n",
    "    \n",
    "    # 跳过复杂模型的样本阈值\n",
    "    skip_tabnet_threshold = 100  # 如果样本少于100，跳过TabNet\n",
    "    skip_tabnet = sample_size < skip_tabnet_threshold\n",
    "    \n",
    "    # 检查极度不平衡数据\n",
    "    fraud_ratio = y.mean()\n",
    "    logger.info(f\"Fraud ratio: {fraud_ratio:.4f} ({y.sum()} fraud cases out of {len(y)} total)\")\n",
    "    \n",
    "    if fraud_ratio < 0.01 or fraud_ratio > 0.99:\n",
    "        logger.warning(f\"Highly imbalanced data: fraud ratio = {fraud_ratio:.4f}\")\n",
    "        # 可以在此处理不平衡问题，如SMOTE或类权重调整\n",
    "    \n",
    "    # 检查最小类别的样本数\n",
    "    min_class_count = min(y.sum(), len(y) - y.sum())\n",
    "    if min_class_count < 5:\n",
    "        logger.error(f\"Insufficient samples for minority class ({min_class_count}). Minimum 5 samples required.\")\n",
    "        logger.error(\"Cannot build reliable model. Skipping industry.\")\n",
    "        return None\n",
    "    \n",
    "    # 数据划分策略\n",
    "    if time_series and date_col is not None and date_col in industry_data.columns:\n",
    "        # 按时间排序\n",
    "        industry_data = industry_data.sort_values(date_col)\n",
    "        \n",
    "        # 时间分割 - 最后20%为测试集\n",
    "        split_idx = int(len(industry_data) * (1 - test_size))\n",
    "        \n",
    "        # 训练+验证集\n",
    "        X_train_val = X.iloc[:split_idx]\n",
    "        y_train_val = y.iloc[:split_idx]\n",
    "        \n",
    "        # 最终测试集 - 只用于最终评估\n",
    "        X_test = X.iloc[split_idx:]\n",
    "        y_test = y.iloc[split_idx:]\n",
    "        \n",
    "        # 从训练+验证集中再次划分训练集和验证集\n",
    "        # 时间分割 - 最后的20%为验证集\n",
    "        val_split_idx = int(len(X_train_val) * 0.8)\n",
    "        X_train = X_train_val.iloc[:val_split_idx]\n",
    "        X_val = X_train_val.iloc[val_split_idx:]\n",
    "        y_train = y_train_val.iloc[:val_split_idx]\n",
    "        y_val = y_train_val.iloc[val_split_idx:]\n",
    "        \n",
    "        logger.info(f\"Time series split: \")\n",
    "        logger.info(f\"  Training set: {len(X_train)} samples ({X_train.index.min()} to {X_train.index.max()})\")\n",
    "        logger.info(f\"  Validation set: {len(X_val)} samples ({X_val.index.min()} to {X_val.index.max()})\")\n",
    "        logger.info(f\"  Test set: {len(X_test)} samples ({X_test.index.min()} to {X_test.index.max()})\")\n",
    "    else:\n",
    "        # 随机分层划分 - 三重划分\n",
    "        # 首先将数据分为训练+验证集和最终测试集\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "        )\n",
    "        \n",
    "        # 然后将训练+验证集进一步分为训练集和验证集\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=0.2, random_state=SEED, stratify=y_train_val\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Random stratified split: \")\n",
    "        logger.info(f\"  Training set: {len(X_train)} samples\")\n",
    "        logger.info(f\"  Validation set: {len(X_val)} samples\")\n",
    "        logger.info(f\"  Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # 检查各个集合的标签分布\n",
    "    logger.info(f\"Training set fraud ratio: {y_train.mean():.4f}\")\n",
    "    logger.info(f\"Validation set fraud ratio: {y_val.mean():.4f}\")\n",
    "    logger.info(f\"Test set fraud ratio: {y_test.mean():.4f}\")\n",
    "    \n",
    "    # 检查测试集大小\n",
    "    if len(X_test) < 30:\n",
    "        logger.warning(f\"Test set is very small ({len(X_test)} samples). Results may be unreliable.\")\n",
    "    \n",
    "    # 增加交叉验证折数处理\n",
    "    if sample_size < 60:\n",
    "        # 小样本量使用留一交叉验证或者5折交叉验证\n",
    "        n_folds = min(5, min_class_count)\n",
    "        logger.info(f\"Using {n_folds}-fold cross-validation due to small sample size\")\n",
    "    else:\n",
    "        # 正常样本量使用5折交叉验证\n",
    "        n_folds = 5\n",
    "    \n",
    "    # 构建模型\n",
    "    try:\n",
    "#         results = build_stacking_model(\n",
    "#             industry=industry,\n",
    "#             X_train=X_train,\n",
    "#             y_train=y_train,\n",
    "#             X_test=X_test,\n",
    "#             y_test=y_test,\n",
    "#             time_series=time_series,\n",
    "#             n_folds=n_folds,\n",
    "#             X_val=X_val if 'X_val' in locals() else None,\n",
    "#             y_val=y_val if 'y_val' in locals() else None,\n",
    "#             use_simplified_models=use_simplified_models,\n",
    "#             skip_tabnet=skip_tabnet,\n",
    "#             target_accuracy=target_accuracy,\n",
    "#             accuracy_margin=accuracy_margin\n",
    "        results = build_stacking_model(\n",
    "            industry=industry,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            time_series=time_series,\n",
    "            n_folds=n_folds,\n",
    "            X_val=X_val if 'X_val' in locals() else None,\n",
    "            y_val=y_val if 'y_val' in locals() else None,\n",
    "            use_simplified_models=use_simplified_models,\n",
    "            skip_tabnet=skip_tabnet,\n",
    "            target_accuracy=target_accuracy,\n",
    "            accuracy_margin=accuracy_margin\n",
    "        )\n",
    "        # 检查可疑的高准确率\n",
    "        if results and results.get('test_auc', 0) > 0.95:\n",
    "            logger.warning(f\"Suspiciously high AUC ({results['test_auc']:.4f}) may indicate data leakage or overfitting.\")\n",
    "            logger.warning(\"Consider these results with caution and perform additional validation.\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing industry {industry}: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ce06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nested_cv_optimize(X, y, base_model, param_grid, cv_outer=5, cv_inner=3, scoring='roc_auc', time_series=False):\n",
    "#     \"\"\"\n",
    "#     使用嵌套交叉验证优化模型参数，避免数据泄露\n",
    "    \n",
    "#     Args:\n",
    "#         X: 特征\n",
    "#         y: 标签\n",
    "#         base_model: 基础模型\n",
    "#         param_grid: 参数网格搜索空间\n",
    "#         cv_outer: 外层交叉验证折数\n",
    "#         cv_inner: 内层交叉验证折数\n",
    "#         scoring: 评分指标\n",
    "#         time_series: 是否使用时间序列分割\n",
    "    \n",
    "#     Returns:\n",
    "#         最佳参数字典、外层交叉验证得分列表和最佳模型\n",
    "#     \"\"\"\n",
    "#     # 修复1: 首先划分一个保留集，避免优化过程接触到最终测试数据\n",
    "#     # 这个保留集将用于独立验证最终选择的参数\n",
    "#     if not time_series:\n",
    "#         X_opt, X_holdout, y_opt, y_holdout = train_test_split(\n",
    "#             X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "#         )\n",
    "#     else:\n",
    "#         # 时间序列数据按时间顺序划分\n",
    "#         split_idx = int(len(X) * 0.8)\n",
    "#         X_opt, X_holdout = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "#         y_opt, y_holdout = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "#     # 选择交叉验证策略\n",
    "#     if time_series:\n",
    "#         cv_outer_split = TimeSeriesSplit(n_splits=cv_outer)\n",
    "#     else:\n",
    "#         cv_outer_split = StratifiedKFold(n_splits=cv_outer, shuffle=True, random_state=SEED)\n",
    "    \n",
    "#     # 外层交叉验证得分\n",
    "#     cv_scores = []\n",
    "#     best_params_list = []\n",
    "#     best_models = []\n",
    "    \n",
    "#     # 外层交叉验证 - 仅使用优化集(X_opt)\n",
    "#     for train_idx, test_idx in cv_outer_split.split(X_opt, y_opt):\n",
    "#         X_train, X_test = X_opt.iloc[train_idx], X_opt.iloc[test_idx]\n",
    "#         y_train, y_test = y_opt.iloc[train_idx], y_opt.iloc[test_idx]\n",
    "        \n",
    "#         # 修复2: 确保标准化过程不会泄露信息\n",
    "#         scaler = StandardScaler()\n",
    "#         X_train_scaled = pd.DataFrame(\n",
    "#             scaler.fit_transform(X_train),\n",
    "#             columns=X_train.columns,\n",
    "#             index=X_train.index\n",
    "#         )\n",
    "#         X_test_scaled = pd.DataFrame(\n",
    "#             scaler.transform(X_test),\n",
    "#             columns=X_test.columns,\n",
    "#             index=X_test.index\n",
    "#         )\n",
    "        \n",
    "#         # 使用Optuna优化参数\n",
    "#         study = optuna.create_study(direction='maximize')\n",
    "        \n",
    "#         # 定义内层交叉验证优化函数\n",
    "#         def objective(trial):\n",
    "#             # 构建参数字典\n",
    "#             params = {}\n",
    "#             for param, values in param_grid.items():\n",
    "#                 if isinstance(values, tuple) and len(values) == 3 and values[0] == 'float':\n",
    "#                     params[param] = trial.suggest_float(param, values[1], values[2])\n",
    "#                 elif isinstance(values, tuple) and len(values) == 3 and values[0] == 'int':\n",
    "#                     params[param] = trial.suggest_int(param, values[1], values[2])\n",
    "#                 elif isinstance(values, tuple) and len(values) == 4 and values[0] == 'float_log':\n",
    "#                     params[param] = trial.suggest_float(param, values[1], values[2], log=values[3])\n",
    "#                 elif isinstance(values, list):\n",
    "#                     params[param] = trial.suggest_categorical(param, values)\n",
    "            \n",
    "#             # 创建模型\n",
    "#             model = clone(base_model)\n",
    "#             model.set_params(**params)\n",
    "            \n",
    "#             # 内层交叉验证 - 使用正确的交叉验证分割器\n",
    "#             if time_series:\n",
    "#                 cv_inner_split = TimeSeriesSplit(n_splits=cv_inner)\n",
    "#             else:\n",
    "#                 cv_inner_split = StratifiedKFold(n_splits=cv_inner, shuffle=True, random_state=SEED)\n",
    "            \n",
    "#             scores = cross_val_score(model, X_train_scaled, y_train, cv=cv_inner_split, scoring=scoring)\n",
    "#             return scores.mean()\n",
    "        \n",
    "#         # 运行参数优化\n",
    "#         study.optimize(objective, n_trials=50)\n",
    "        \n",
    "#         # 获取最佳参数\n",
    "#         best_params = study.best_params\n",
    "#         best_params_list.append(best_params)\n",
    "        \n",
    "#         # 使用最佳参数训练模型\n",
    "#         best_model = clone(base_model)\n",
    "#         best_model.set_params(**best_params)\n",
    "#         best_model.fit(X_train_scaled, y_train)\n",
    "#         best_models.append(best_model)\n",
    "        \n",
    "#         # 在测试集上评估\n",
    "#         if hasattr(best_model, 'predict_proba'):\n",
    "#             y_pred = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "#         else:\n",
    "#             y_pred = best_model.predict(X_test_scaled)\n",
    "        \n",
    "#         # 计算得分\n",
    "#         score = roc_auc_score(y_test, y_pred)\n",
    "#         cv_scores.append(score)\n",
    "        \n",
    "#         logger.info(f\"Fold score: {score:.4f}, Best params: {best_params}\")\n",
    "    \n",
    "#     # 计算最佳参数\n",
    "#     best_params_combined = {}\n",
    "#     for param in best_params_list[0].keys():\n",
    "#         param_values = [bp[param] for bp in best_params_list]\n",
    "#         if all(isinstance(x, (int, float)) for x in param_values):\n",
    "#             best_params_combined[param] = sum(param_values) / len(param_values)\n",
    "#             if all(isinstance(x, int) for x in param_values):\n",
    "#                 best_params_combined[param] = int(best_params_combined[param])\n",
    "#         else:\n",
    "#             # 对于分类参数，使用众数\n",
    "#             from collections import Counter\n",
    "#             best_params_combined[param] = Counter(param_values).most_common(1)[0][0]\n",
    "    \n",
    "#     # 修复3: 使用保留集进行最终验证\n",
    "#     # 标准化保留集\n",
    "#     holdout_scaler = StandardScaler()\n",
    "#     X_holdout_scaled = pd.DataFrame(\n",
    "#         holdout_scaler.fit_transform(X_holdout),\n",
    "#         columns=X_holdout.columns,\n",
    "#         index=X_holdout.index\n",
    "#     )\n",
    "    \n",
    "#     # 训练一个使用最佳参数的模型\n",
    "#     final_model = clone(base_model)\n",
    "#     final_model.set_params(**best_params_combined)\n",
    "    \n",
    "#     # 在所有优化数据上训练\n",
    "#     X_opt_scaled = pd.DataFrame(\n",
    "#         holdout_scaler.transform(X_opt),  # 使用相同的scaler\n",
    "#         columns=X_opt.columns,\n",
    "#         index=X_opt.index\n",
    "#     )\n",
    "#     final_model.fit(X_opt_scaled, y_opt)\n",
    "    \n",
    "#     # 在保留集上验证\n",
    "#     if hasattr(final_model, 'predict_proba'):\n",
    "#         holdout_pred = final_model.predict_proba(X_holdout_scaled)[:, 1]\n",
    "#     else:\n",
    "#         holdout_pred = final_model.predict(X_holdout_scaled)\n",
    "    \n",
    "#     # 计算保留集得分\n",
    "#     holdout_score = roc_auc_score(y_holdout, holdout_pred)\n",
    "#     logger.info(f\"Final holdout score: {holdout_score:.4f}\")\n",
    "    \n",
    "#     # 比较交叉验证平均分数与保留集分数\n",
    "#     cv_mean = np.mean(cv_scores)\n",
    "#     logger.info(f\"CV mean score: {cv_mean:.4f}, Holdout score: {holdout_score:.4f}\")\n",
    "#     if abs(cv_mean - holdout_score) > 0.1:  # 差异超过0.1\n",
    "#         logger.warning(f\"Large gap between CV ({cv_mean:.4f}) and holdout ({holdout_score:.4f}) scores!\")\n",
    "#         logger.warning(\"This may indicate potential overfitting or data leakage.\")\n",
    "    \n",
    "#     # 返回最佳参数、得分和最佳模型\n",
    "#     return best_params_combined, cv_scores, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebb5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lgbm(X, y, time_series=False, use_simplified_models=False):\n",
    "    \"\"\"\n",
    "    优化LightGBM模型超参数，根据样本量自动调整模型复杂度\n",
    "    \n",
    "    Args:\n",
    "        X: 特征矩阵\n",
    "        y: 目标变量\n",
    "        time_series: 是否为时间序列数据\n",
    "        use_simplified_models: 是否使用简化模型（样本数量少时）\n",
    "        \n",
    "    Returns:\n",
    "        最佳参数字典和训练好的模型列表\n",
    "    \"\"\"\n",
    "    # 定义超参数搜索空间\n",
    "    if use_simplified_models:\n",
    "        # 简化模型 - 减少深度和叶节点，增加正则化\n",
    "        param_grid = {\n",
    "            'num_leaves': ('int', 8, 31),  # 减少叶节点数\n",
    "            'max_depth': ('int', 3, 5),    # 减少树深度\n",
    "            'learning_rate': ('float', 0.01, 0.1),\n",
    "            'min_child_samples': ('int', 10, 30),  # 增加每个叶节点最小样本数\n",
    "            'subsample': ('float', 0.7, 0.9),      # 增加行抽样比例，减少随机性\n",
    "            'colsample_bytree': ('float', 0.7, 0.9), # 增加列抽样比例\n",
    "            'reg_alpha': ('float', 0.1, 1.0),      # 增强L1正则化\n",
    "            'reg_lambda': ('float', 0.1, 1.0),     # 增强L2正则化\n",
    "            'min_split_gain': ('float', 0.1, 0.5)  # 增加分裂增益阈值，减少过拟合\n",
    "        }\n",
    "        n_trials = 20  # 减少搜索次数\n",
    "    else:\n",
    "        # 完整模型 - 正常参数范围\n",
    "        param_grid = {\n",
    "            'num_leaves': ('int', 15, 255),\n",
    "            'max_depth': ('int', 3, 12),\n",
    "            'learning_rate': ('float', 0.01, 0.2),\n",
    "            'min_child_samples': ('int', 5, 100),\n",
    "            'subsample': ('float', 0.5, 1.0),\n",
    "            'colsample_bytree': ('float', 0.5, 1.0),\n",
    "            'reg_alpha': ('float', 0, 10),\n",
    "            'reg_lambda': ('float', 0, 10),\n",
    "            'min_split_gain': ('float', 0, 1)\n",
    "        }\n",
    "        n_trials = 50  # 正常搜索次数\n",
    "    \n",
    "    # 检查样本量，进一步调整参数\n",
    "    if len(X) < 50:\n",
    "        # 极小样本量，进一步简化模型\n",
    "        param_grid['max_depth'] = ('int', 2, 4)  # 更浅的树\n",
    "        param_grid['num_leaves'] = ('int', 4, 16)  # 更少的叶节点\n",
    "        param_grid['min_child_samples'] = ('int', 5, 15)  # 更小的叶节点样本要求\n",
    "        param_grid['reg_alpha'] = ('float', 0.5, 2.0)  # 更强的正则化\n",
    "        param_grid['reg_lambda'] = ('float', 0.5, 2.0)  # 更强的正则化\n",
    "        n_trials = 15  # 进一步减少搜索次数\n",
    "        \n",
    "    # 使用交叉验证\n",
    "    if time_series:\n",
    "        cv = TimeSeriesSplit(n_splits=min(5, max(3, len(X) // 10)))  # 根据样本量动态调整折数\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=min(5, max(3, len(X) // 10)), shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # 创建优化函数\n",
    "    def objective(trial):\n",
    "        # 构建参数\n",
    "        params = {}\n",
    "        for param, values in param_grid.items():\n",
    "            if values[0] == 'int':\n",
    "                params[param] = trial.suggest_int(param, values[1], values[2])\n",
    "            elif values[0] == 'float':\n",
    "                params[param] = trial.suggest_float(param, values[1], values[2])\n",
    "        \n",
    "        # 添加固定参数\n",
    "        params['objective'] = 'binary'\n",
    "        params['verbosity'] = -1\n",
    "        params['random_state'] = SEED\n",
    "        \n",
    "        # 创建模型\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        # 使用交叉验证评估\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        # 返回平均分数\n",
    "        return scores.mean()\n",
    "    \n",
    "    # 创建Optuna研究\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # 减少日志输出\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    # 运行优化\n",
    "    logger.info(f\"Optimizing LightGBM with {n_trials} trials{'(simplified model)' if use_simplified_models else ''}\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # 获取最佳参数\n",
    "    best_params = study.best_params\n",
    "    # 不要在best_params中添加'objective'参数\n",
    "    \n",
    "    # 记录最佳参数\n",
    "    logger.info(f\"LightGBM best params: {best_params}\")\n",
    "    logger.info(f\"LightGBM best score: {study.best_value:.4f}\")\n",
    "    \n",
    "    # 训练最终模型 - 在这里可以添加'objective'参数\n",
    "    final_models = []\n",
    "    final_model_params = best_params.copy()\n",
    "    final_model_params['objective'] = 'binary'  # 只在这里使用\n",
    "    final_model_params['verbosity'] = -1\n",
    "    final_model_params['random_state'] = SEED\n",
    "    \n",
    "    # 拟合全部数据\n",
    "    final_model = lgb.LGBMClassifier(**final_model_params)\n",
    "    final_model.fit(X, y)\n",
    "    final_models.append(final_model)\n",
    "    \n",
    "    # 返回最佳参数和模型\n",
    "    return best_params, final_models  # 返回不包含'objective'的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b560949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgb(X, y, time_series=False, use_simplified_models=False):\n",
    "    \"\"\"\n",
    "    优化XGBoost模型参数\n",
    "    \"\"\"\n",
    "    import xgboost as xgb\n",
    "    # 显式重置XGBoost全局配置\n",
    "    xgb.config.set_config(verbosity=0)\n",
    "    \n",
    "    # 首先确保XGBoost的全局配置是正确的\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        # 显式重置XGBoost全局配置\n",
    "        xgb.config.set_config(verbosity=0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"设置XGBoost全局配置时出错: {str(e)}\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'alpha': trial.suggest_float('alpha', 0, 10),\n",
    "            'lambda': trial.suggest_float('lambda', 0, 10),\n",
    "            'objective': 'binary:logistic',  # 使用正确的objective值\n",
    "            'verbosity': 0,  # 确保在有效范围0-3内\n",
    "            'use_label_encoder': False,  # 避免警告\n",
    "            'eval_metric': 'logloss',  # 添加明确的评估指标\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        if use_simplified_models:\n",
    "            # 简化模型参数，降低复杂度\n",
    "            params['max_depth'] = trial.suggest_int('max_depth', 2, 6)\n",
    "            params['min_child_weight'] = trial.suggest_int('min_child_weight', 1, 5)\n",
    "            params['subsample'] = trial.suggest_float('subsample', 0.7, 1.0)\n",
    "            params['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.7, 1.0)\n",
    "        \n",
    "        try:\n",
    "            if time_series:\n",
    "                # 时间序列数据使用时间分割验证\n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                score = cross_val_score(model, X, y, cv=tscv, scoring='accuracy').mean()\n",
    "            else:\n",
    "                # 非时间序列数据使用分层K折交叉验证\n",
    "                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                score = cross_val_score(model, X, y, cv=skf, scoring='accuracy').mean()\n",
    "                \n",
    "            return score\n",
    "        except Exception as e:\n",
    "            logger.error(f\"交叉验证出错: {str(e)}\")\n",
    "            return 0.0  # 返回最低分数以避免中断优化过程\n",
    "    \n",
    "    try:\n",
    "        # 使用Optuna优化XGBoost参数\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=50)\n",
    "        \n",
    "        # 获取最佳参数并训练最终模型\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': 42\n",
    "        })\n",
    "        \n",
    "        # 记录最佳参数和得分\n",
    "        logger.info(f\"XGBoost best params: {best_params}\")\n",
    "        logger.info(f\"XGBoost best score: {study.best_value:.4f}\")\n",
    "        \n",
    "        # 使用最佳参数训练模型\n",
    "        final_model = xgb.XGBClassifier(**best_params)\n",
    "        final_model.fit(X, y)\n",
    "        \n",
    "        # 返回最佳参数和训练好的模型\n",
    "        return best_params, final_model\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"XGBoost优化过程出错: {str(e)}\")\n",
    "        # 使用默认参数作为备选\n",
    "        default_params = {\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'min_child_weight': 1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'gamma': 0,\n",
    "            'alpha': 0,\n",
    "            'lambda': 1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        logger.info(\"使用默认XGBoost参数\")\n",
    "        try:\n",
    "            # 在这里创建一个新的XGBClassifier而不是使用旧的实例\n",
    "            xgb.config.set_config(verbosity=0)  # 再次确保全局配置正确\n",
    "            final_model = xgb.XGBClassifier(**default_params)\n",
    "            final_model.fit(X, y)\n",
    "            return default_params, final_model\n",
    "        except Exception as inner_e:\n",
    "            logger.error(f\"使用默认参数训练XGBoost时出错: {str(inner_e)}\")\n",
    "            # 可能需要回退到其他类型的模型\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            logger.info(\"回退到RandomForest模型\")\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X, y)\n",
    "            return {\"fallback_to\": \"RandomForest\"}, rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b3659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_tabnet(X, y, time_series=False, use_simplified_models=False):\n",
    "    \"\"\"\n",
    "    优化TabNet模型超参数，根据样本量自动调整模型复杂度\n",
    "    \n",
    "    Args:\n",
    "        X: 特征矩阵\n",
    "        y: 目标变量\n",
    "        time_series: 是否为时间序列数据\n",
    "        use_simplified_models: 是否使用简化模型（样本数量少时）\n",
    "        \n",
    "    Returns:\n",
    "        最佳参数字典和训练好的模型列表\n",
    "    \"\"\"\n",
    "    # 定义超参数搜索空间\n",
    "    if use_simplified_models:\n",
    "        # 简化模型 - 减少网络复杂性、步骤和稀疏性\n",
    "        param_grid = {\n",
    "            'n_d': ('int', 8, 32),              # 减少特征转换维度\n",
    "            'n_a': ('int', 8, 32),              # 减少注意力维度\n",
    "            'n_steps': ('int', 2, 4),           # 减少决策步骤数\n",
    "            'gamma': ('float', 1.0, 1.5),       # 减少特征选择熵正则化参数\n",
    "            'momentum': ('float', 0.01, 0.3),   # 动量范围\n",
    "            'lambda_sparse': ('float_log', 1e-5, 1e-2, True), # 增加稀疏性约束，减少过拟合\n",
    "            'n_independent': ('int', 1, 2),      # 减少独立网络数量\n",
    "            'n_shared': ('int', 1, 2)            # 减少共享网络数量\n",
    "        }\n",
    "        n_trials = 15  # 减少搜索次数\n",
    "        epochs = 50    # 减少训练轮数\n",
    "        patience = 10  # 减少早停耐心值\n",
    "    else:\n",
    "        # 完整模型 - 正常参数范围\n",
    "        param_grid = {\n",
    "            'n_d': ('int', 16, 64),\n",
    "            'n_a': ('int', 16, 64),\n",
    "            'n_steps': ('int', 3, 10),\n",
    "            'gamma': ('float', 1.0, 2.0),\n",
    "            'momentum': ('float', 0.01, 0.4),\n",
    "            'lambda_sparse': ('float_log', 1e-6, 1e-3, True),\n",
    "            'n_independent': ('int', 1, 5),\n",
    "            'n_shared': ('int', 1, 5)\n",
    "        }\n",
    "        n_trials = 30  # 正常搜索次数\n",
    "        epochs = 100   # 正常训练轮数\n",
    "        patience = 20  # 正常早停耐心值\n",
    "    \n",
    "    # 检查样本量，进一步调整参数\n",
    "    if len(X) < 50:\n",
    "        # 极小样本量，进一步简化模型\n",
    "        param_grid['n_d'] = ('int', 4, 16)  # 更小的特征转换维度\n",
    "        param_grid['n_a'] = ('int', 4, 16)  # 更小的注意力维度\n",
    "        param_grid['n_steps'] = ('int', 1, 3)  # 更少的决策步骤\n",
    "        param_grid['n_independent'] = ('int', 1, 1)  # 固定独立网络数量为1\n",
    "        param_grid['n_shared'] = ('int', 1, 1)  # 固定共享网络数量为1\n",
    "        param_grid['lambda_sparse'] = ('float_log', 1e-4, 1e-2, True)  # 更强的稀疏性约束\n",
    "        n_trials = 10  # 进一步减少搜索次数\n",
    "        epochs = 30    # 进一步减少训练轮数\n",
    "        patience = 5   # 进一步减少早停耐心值\n",
    "        \n",
    "    # 使用交叉验证\n",
    "    if time_series:\n",
    "        # 时间序列分割\n",
    "        n_splits = min(5, max(3, len(X) // 10))  # 根据样本量动态调整折数\n",
    "        cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    else:\n",
    "        # 分层k折交叉验证\n",
    "        n_splits = min(5, max(3, len(X) // 10))  # 根据样本量动态调整折数\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # 由于TabNet不容易集成到sklearn的CV流程中，我们手动实现交叉验证\n",
    "    def objective(trial):\n",
    "        # 构建参数\n",
    "        params = {}\n",
    "        for param, values in param_grid.items():\n",
    "            if values[0] == 'int':\n",
    "                params[param] = trial.suggest_int(param, values[1], values[2])\n",
    "            elif values[0] == 'float':\n",
    "                params[param] = trial.suggest_float(param, values[1], values[2])\n",
    "            elif values[0] == 'float_log':\n",
    "                params[param] = trial.suggest_float(param, values[1], values[2], log=values[3])\n",
    "        \n",
    "        # 初始化分数存储\n",
    "        scores = []\n",
    "        \n",
    "        # 创建标准化器\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # 交叉验证\n",
    "        for train_idx, valid_idx in cv.split(X_scaled, y):\n",
    "            X_train_fold, X_valid_fold = X_scaled[train_idx], X_scaled[valid_idx]\n",
    "            y_train_fold, y_valid_fold = y.iloc[train_idx].values, y.iloc[valid_idx].values\n",
    "            \n",
    "            # 创建模型\n",
    "            model = TabNetClassifier(\n",
    "                **params,\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params=dict(lr=0.02),\n",
    "                scheduler_params={\"mode\": \"min\", \"factor\": 0.7, \"patience\": 5},\n",
    "                scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                seed=SEED,\n",
    "                device_name=device\n",
    "            )\n",
    "            \n",
    "            # 训练模型\n",
    "            model.fit(\n",
    "                X_train=X_train_fold, \n",
    "                y_train=y_train_fold,\n",
    "                eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                max_epochs=epochs,\n",
    "                patience=patience,\n",
    "                batch_size=min(1024, len(X_train_fold)),  # 调整batch size避免小样本问题\n",
    "                virtual_batch_size=min(128, len(X_train_fold) // 4),  # 调整虚拟batch size\n",
    "                num_workers=0,\n",
    "                drop_last=False\n",
    "            )\n",
    "            \n",
    "            # 评估模型\n",
    "            y_pred = model.predict_proba(X_valid_fold)[:, 1]\n",
    "            score = roc_auc_score(y_valid_fold, y_pred)\n",
    "            scores.append(score)\n",
    "            \n",
    "            # 清理内存\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # 返回平均分数\n",
    "        return np.mean(scores)\n",
    "    \n",
    "    # 创建Optuna研究\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # 减少日志输出\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    # 运行优化\n",
    "    logger.info(f\"Optimizing TabNet with {n_trials} trials{'(simplified model)' if use_simplified_models else ''}\")\n",
    "    \n",
    "    try:\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        # 获取最佳参数\n",
    "        best_params = study.best_params\n",
    "        \n",
    "        # 记录最佳参数\n",
    "        logger.info(f\"TabNet best params: {best_params}\")\n",
    "        logger.info(f\"TabNet CV scores: {study.trials_dataframe()['value'].tolist()}, Mean: {study.best_value:.4f}, Std: {study.trials_dataframe()['value'].std():.4f}\")\n",
    "        \n",
    "        # 训练最终模型\n",
    "        final_model = TabNetClassifier(\n",
    "            **best_params,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=0.02),\n",
    "            scheduler_params={\"mode\": \"min\", \"factor\": 0.7, \"patience\": 5},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            seed=SEED,\n",
    "            device_name=device\n",
    "        )\n",
    "        \n",
    "        # 不在这里拟合最终模型，因为在build_stacking_model中会进行拟合\n",
    "        final_models = []\n",
    "        \n",
    "        # 返回最佳参数和模型\n",
    "        return best_params, final_models\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error optimizing TabNet: {e}\", exc_info=True)\n",
    "        # 如果优化失败，返回默认参数\n",
    "        default_params = {\n",
    "            'n_d': 16 if not use_simplified_models else 8,\n",
    "            'n_a': 16 if not use_simplified_models else 8,\n",
    "            'n_steps': 3 if not use_simplified_models else 2,\n",
    "            'gamma': 1.3,\n",
    "            'momentum': 0.02,\n",
    "            'lambda_sparse': 0.001 if not use_simplified_models else 0.01,\n",
    "            'n_independent': 2 if not use_simplified_models else 1,\n",
    "            'n_shared': 2 if not use_simplified_models else 1\n",
    "        }\n",
    "        logger.warning(f\"Using default TabNet parameters: {default_params}\")\n",
    "        return default_params, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6fedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, industry_name):\n",
    "    \"\"\"\n",
    "    保存模型到文件\n",
    "    \n",
    "    Args:\n",
    "        model: 完整的模型对象\n",
    "        industry_name: 行业名称\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    import joblib\n",
    "    \n",
    "    # 创建模型目录（如果不存在）\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # 生成模型文件名\n",
    "    model_pkl_path = f'models/{industry_name}_stacking_model.pkl'\n",
    "    model_joblib_path = f'models/{industry_name}_stacking_model.joblib'\n",
    "    \n",
    "    try:\n",
    "        # 使用pickle保存\n",
    "        with open(model_pkl_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        logger.info(f\"Model saved to: {model_pkl_path}\")\n",
    "        \n",
    "        # 同时也使用joblib保存（通常更可靠，尤其是对于大型模型）\n",
    "        joblib.dump(model, model_joblib_path)\n",
    "        logger.info(f\"Model saved to: {model_joblib_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model for {industry_name}: {e}\", exc_info=True)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76f80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacking_model(industry, X_train, y_train, X_test, y_test, time_series=False, n_folds=5, \n",
    "                   X_val=None, y_val=None, use_simplified_models=False, skip_tabnet=False,\n",
    "                   target_accuracy=0.89, accuracy_margin=0.01):\n",
    "    \"\"\"\n",
    "    构建堆叠模型，避免数据泄露，计算并使用最佳阈值，根据样本量自动调整模型复杂度\n",
    "    增加了目标准确率早停机制，在达到目标准确率附近时停止训练\n",
    "    \n",
    "    Args:\n",
    "        industry: 行业名称\n",
    "        X_train: 训练特征\n",
    "        y_train: 训练标签\n",
    "        X_test: 测试特征\n",
    "        y_test: 测试标签\n",
    "        time_series: 是否按时间序列处理\n",
    "        n_folds: 交叉验证折数\n",
    "        X_val: 验证集特征 (可选)\n",
    "        y_val: 验证集标签 (可选)\n",
    "        use_simplified_models: 是否使用简化模型 (样本数量少时)\n",
    "        skip_tabnet: 是否跳过TabNet模型 (样本数量少时)\n",
    "        target_accuracy: 目标准确率，达到此准确率附近时停止训练\n",
    "        accuracy_margin: 准确率容差范围\n",
    "    \n",
    "    Returns:\n",
    "        模型结果字典\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n=============== Building Stacking Model for {industry} ===============\")\n",
    "    logger.info(f\"Target accuracy: {target_accuracy:.4f} (±{accuracy_margin:.4f})\")\n",
    "    results = {}\n",
    "    trained_base_models = {}\n",
    "    \n",
    "    # 检查样本是否极度不平衡\n",
    "    fraud_ratio = y_train.mean()\n",
    "    if fraud_ratio < 0.01 or fraud_ratio > 0.99:\n",
    "        logger.warning(f\"Highly imbalanced data: fraud ratio = {fraud_ratio:.4f}\")\n",
    "        # 设置类权重\n",
    "        class_weights = {\n",
    "            0: 1, \n",
    "            1: (1-fraud_ratio)/fraud_ratio if fraud_ratio < 0.5 else fraud_ratio/(1-fraud_ratio)\n",
    "        }\n",
    "        logger.info(f\"Using class weights: {class_weights}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    # 日志记录模型复杂度设置\n",
    "    if use_simplified_models:\n",
    "        logger.info(\"Using simplified models due to small sample size\")\n",
    "    if skip_tabnet:\n",
    "        logger.info(\"Skipping TabNet model due to small sample size\")\n",
    "    \n",
    "    # 第一步：优化各个基模型的超参数\n",
    "    lgbm_params, lgbm_models = optimize_lgbm(X_train, y_train, time_series=time_series, use_simplified_models=use_simplified_models)\n",
    "    xgb_params, xgb_models = optimize_xgb(X_train, y_train, time_series=time_series, use_simplified_models=use_simplified_models)\n",
    "    \n",
    "    # 只在样本足够时使用TabNet\n",
    "    if not skip_tabnet:\n",
    "        tabnet_params, _ = optimize_tabnet(X_train, y_train, time_series=time_series, use_simplified_models=use_simplified_models)\n",
    "    else:\n",
    "        # 跳过TabNet时设置空参数\n",
    "        tabnet_params = {}\n",
    "    \n",
    "    # 第二步：创建交叉验证折\n",
    "    if time_series:\n",
    "        kf = TimeSeriesSplit(n_splits=n_folds)\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # 创建存储交叉验证预测结果的数组\n",
    "    # 如果跳过TabNet，则只使用2个基模型\n",
    "    n_base_models = 2 if skip_tabnet else 3\n",
    "    cv_train_meta = np.zeros((X_train.shape[0], n_base_models))\n",
    "    \n",
    "    # 存储训练好的模型和标准化器\n",
    "    trained_models = {\n",
    "        'lgbm': [],\n",
    "        'xgb': [],\n",
    "        'tabnet': [] if not skip_tabnet else None\n",
    "    }\n",
    "    \n",
    "    trained_scalers = []\n",
    "\n",
    "\n",
    "    # 在训练数据上拟合标准化器\n",
    "    train_scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        train_scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "\n",
    "    # 将训练数据上拟合的标准化器应用到测试数据\n",
    "    X_test_scaled_global = pd.DataFrame(\n",
    "        train_scaler.transform(X_test),  # 只transform，不fit\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # 为存储测试集上每个模型的预测结果创建数组\n",
    "    test_meta_features = np.zeros((X_test.shape[0], n_base_models))\n",
    "    \n",
    "    # 创建早停标志和当前准确率跟踪\n",
    "    early_stopping_triggered = False\n",
    "    current_accuracy = 0\n",
    "    accuracy_history = []  # 存储每个fold的准确率\n",
    "    \n",
    "    # 第三步：通过交叉验证训练基模型并生成元特征\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "#         if early_stopping_triggered:\n",
    "#             logger.info(f\"Early stopping triggered at fold {fold_idx}/{n_folds}: accuracy {current_accuracy:.4f} is within target range\")\n",
    "#             break\n",
    "            \n",
    "        logger.info(f\"\\nTraining fold {fold_idx+1}/{n_folds}\")\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "        \n",
    "        # 对训练集进行标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train_fold), \n",
    "            columns=X_train_fold.columns,\n",
    "            index=X_train_fold.index\n",
    "        )\n",
    "        \n",
    "        # 保存标准化器\n",
    "        trained_scalers.append(scaler)\n",
    "        \n",
    "        # 使用相同的scaler转换验证集\n",
    "        X_valid_fold_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_valid_fold), \n",
    "            columns=X_valid_fold.columns,\n",
    "            index=X_valid_fold.index\n",
    "        )\n",
    "        \n",
    "        # 训练LightGBM并获取预测\n",
    "        logger.info(\"Training LightGBM...\")\n",
    "        lgbm_params_complete = lgbm_params.copy()\n",
    "        lgbm_params_complete['objective'] = 'binary'\n",
    "        lgbm_params_complete['verbosity'] = -1\n",
    "        lgbm_params_complete['random_state'] = SEED\n",
    "\n",
    "        if class_weights:\n",
    "            lgbm_params_complete['class_weight'] = class_weights\n",
    "\n",
    "        lgbm_model = lgb.LGBMClassifier(**lgbm_params_complete, n_jobs=-1)\n",
    "        lgbm_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "        cv_train_meta[valid_idx, 0] = lgbm_model.predict_proba(X_valid_fold_scaled)[:, 1]\n",
    "        trained_models['lgbm'].append(lgbm_model)\n",
    "        \n",
    "        # 训练XGBoost并获取预测\n",
    "#         logger.info(\"Training XGBoost...\")\n",
    "#         xgb_params_complete = xgb_params.copy()\n",
    "#         xgb_params_complete['objective'] = 'binary:logistic'\n",
    "#         xgb_params_complete['eval_metric'] = 'auc'\n",
    "#         xgb_params_complete['use_label_encoder'] = False\n",
    "#         xgb_params_complete['random_state'] = SEED\n",
    "\n",
    "        # 重置XGBoost全局配置\n",
    "        try:\n",
    "            import xgboost as xgb\n",
    "            xgb.config.set_config(verbosity=0)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"无法重置XGBoost全局配置: {str(e)}\")\n",
    "\n",
    "        # 在训练XGBoost的部分做如下修改\n",
    "        logger.info(\"Training XGBoost...\")\n",
    "        xgb_params_complete = xgb_params.copy()\n",
    "        xgb_params_complete['objective'] = 'binary:logistic'\n",
    "        xgb_params_complete['eval_metric'] = 'auc'\n",
    "        xgb_params_complete['use_label_encoder'] = False\n",
    "        xgb_params_complete['random_state'] = SEED\n",
    "        # 确保verbosity设置正确\n",
    "        xgb_params_complete['verbosity'] = 0  # 确保在0-3范围内\n",
    "\n",
    "        if class_weights and fraud_ratio < 0.5:\n",
    "            # XGBoost使用scale_pos_weight参数\n",
    "            xgb_params_complete['scale_pos_weight'] = class_weights[1]\n",
    "\n",
    "        xgb_model = xgb.XGBClassifier(**xgb_params_complete, n_jobs=-1)\n",
    "        xgb_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "        cv_train_meta[valid_idx, 1] = xgb_model.predict_proba(X_valid_fold_scaled)[:, 1]\n",
    "        trained_models['xgb'].append(xgb_model)\n",
    "        \n",
    "        # 只在不跳过TabNet时训练TabNet\n",
    "        if not skip_tabnet:\n",
    "            # 训练TabNet并获取预测\n",
    "            logger.info(\"Training TabNet...\")\n",
    "            tabnet_model = TabNetClassifier(\n",
    "                **tabnet_params,\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params=dict(lr=0.02),\n",
    "                scheduler_params={\"mode\": \"min\", \"factor\": 0.7, \"patience\": 5},\n",
    "                scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                seed=SEED,\n",
    "                device_name=device\n",
    "            )\n",
    "            \n",
    "            # 计算类权重（如果需要）\n",
    "            if class_weights and fraud_ratio < 0.5:\n",
    "                weights = np.ones(len(y_train_fold))\n",
    "                weights[y_train_fold == 1] = class_weights[1]\n",
    "\n",
    "                # 确保权重的格式正确 - 使用numpy数组\n",
    "                sample_weights = np.array(weights)\n",
    "\n",
    "                # 确保权重与训练样本长度一致\n",
    "                assert len(sample_weights) == len(y_train_fold), \"Weights length doesn't match training samples\"\n",
    "\n",
    "                # 使用带权重的训练\n",
    "                tabnet_model.fit(\n",
    "                    X_train=X_train_fold_scaled.values, \n",
    "                    y_train=y_train_fold.values,\n",
    "                    eval_set=[(X_valid_fold_scaled.values, y_valid_fold.values)],\n",
    "                    max_epochs=100 if not use_simplified_models else 50,  # 简化模型时减少epoch数\n",
    "                    patience=20 if not use_simplified_models else 10,    # 简化模型时提前停止\n",
    "                    batch_size=1024,\n",
    "                    virtual_batch_size=128,\n",
    "                    num_workers=0,\n",
    "                    drop_last=False,\n",
    "                    weights=sample_weights\n",
    "                )\n",
    "            else:\n",
    "                # 不使用权重训练\n",
    "                tabnet_model.fit(\n",
    "                    X_train=X_train_fold_scaled.values, \n",
    "                    y_train=y_train_fold.values,\n",
    "                    eval_set=[(X_valid_fold_scaled.values, y_valid_fold.values)],\n",
    "                    max_epochs=100 if not use_simplified_models else 50,  # 简化模型时减少epoch数\n",
    "                    patience=20 if not use_simplified_models else 10,    # 简化模型时提前停止\n",
    "                    batch_size=1024,\n",
    "                    virtual_batch_size=128,\n",
    "                    num_workers=0,\n",
    "                    drop_last=False\n",
    "                )\n",
    "            \n",
    "            # 分批处理TabNet预测以减少内存使用\n",
    "            batch_size = 2048\n",
    "            \n",
    "            # 验证集预测\n",
    "            num_valid_samples = X_valid_fold_scaled.shape[0]\n",
    "            valid_preds = []\n",
    "            for i in range(0, num_valid_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, num_valid_samples)\n",
    "                batch_preds = tabnet_model.predict_proba(X_valid_fold_scaled.iloc[i:end_idx].values)[:, 1]\n",
    "                valid_preds.append(batch_preds)\n",
    "            cv_train_meta[valid_idx, 2] = np.concatenate(valid_preds)\n",
    "            \n",
    "            trained_models['tabnet'].append(tabnet_model)\n",
    "        \n",
    "        # 清理内存\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # 早停检查 - 使用当前fold的验证集计算准确率\n",
    "        # 注意：这里我们使用单个fold的验证集，而不是完整的测试集\n",
    "        # 构建当前fold的元特征\n",
    "        current_fold_meta = np.zeros((len(valid_idx), n_base_models))\n",
    "        current_fold_meta[:, 0] = lgbm_model.predict_proba(X_valid_fold_scaled)[:, 1]\n",
    "        current_fold_meta[:, 1] = xgb_model.predict_proba(X_valid_fold_scaled)[:, 1]\n",
    "        \n",
    "        if not skip_tabnet:\n",
    "            # TabNet预测已经计算过了\n",
    "            current_fold_meta[:, 2] = np.concatenate(valid_preds)\n",
    "        \n",
    "        # 使用简单的LR作为元模型进行快速检查\n",
    "        meta_model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "        meta_model.fit(current_fold_meta, y_valid_fold)\n",
    "        \n",
    "        # 预测并计算准确率\n",
    "#         fold_predictions = meta_model.predict(current_fold_meta)\n",
    "#         from sklearn.metrics import accuracy_score\n",
    "#         fold_accuracy = accuracy_score(y_valid_fold, fold_predictions)\n",
    "#         current_accuracy = fold_accuracy\n",
    "        fold_predictions = meta_model.predict(current_fold_meta)\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        fold_accuracy = accuracy_score(y_valid_fold, fold_predictions)\n",
    "        current_accuracy = fold_accuracy\n",
    "        accuracy_history.append(fold_accuracy)\n",
    "        \n",
    "        logger.info(f\"Fold {fold_idx+1} accuracy: {fold_accuracy:.4f}\")\n",
    "        \n",
    "#         # 检查是否达到目标准确率\n",
    "#         if abs(fold_accuracy - target_accuracy) <= accuracy_margin:\n",
    "#             logger.info(f\"Reached target accuracy range: {fold_accuracy:.4f} is within {target_accuracy:.4f}±{accuracy_margin:.4f}\")\n",
    "#             early_stopping_triggered = True\n",
    "#         elif fold_accuracy > target_accuracy + accuracy_margin:\n",
    "#             logger.info(f\"Exceeded target accuracy: {fold_accuracy:.4f} > {target_accuracy:.4f}+{accuracy_margin:.4f}\")\n",
    "#             early_stopping_triggered = True\n",
    "        # 更积极的早停策略\n",
    "        # 1. 不等待精确匹配，使用下限作为触发条件\n",
    "        # 2. 只要达到或超过目标准确率下限就立即停止\n",
    "        if fold_accuracy >= target_accuracy - accuracy_margin:\n",
    "            logger.info(f\"Reached minimum target accuracy threshold: {fold_accuracy:.4f} >= {target_accuracy-accuracy_margin:.4f}\")\n",
    "            logger.info(\"Triggering early stopping\")\n",
    "            early_stopping_triggered = True\n",
    "            break\n",
    "\n",
    "        # 清理内存\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "#     # 如果早停被触发，记录\n",
    "#     if early_stopping_triggered:\n",
    "#         logger.info(f\"Early stopping triggered at accuracy {current_accuracy:.4f}\")\n",
    "    \n",
    "    # 如果早停被触发，记录\n",
    "    if early_stopping_triggered:\n",
    "        logger.info(f\"Early stopping triggered at accuracy {current_accuracy:.4f}\")\n",
    "        logger.info(f\"Completed {fold_idx+1} of {n_folds} folds\")\n",
    "    else:\n",
    "        logger.info(f\"Completed all {n_folds} folds without triggering early stopping\")\n",
    "    \n",
    "    # 在测试集上进行预测\n",
    "    # 修复2: 在所有fold训练完成后，使用所有训练好的模型对测试集进行一次预测\n",
    "    logger.info(\"\\nGenerating test set predictions from all models...\")\n",
    "    \n",
    "    # 对于每个模型类型，使用所有fold的模型进行预测，然后取平均值\n",
    "    # LightGBM模型预测\n",
    "    lgbm_preds = []\n",
    "    for model in trained_models['lgbm']:\n",
    "        lgbm_preds.append(model.predict_proba(X_test_scaled_global)[:, 1])\n",
    "    test_meta_features[:, 0] = np.mean(lgbm_preds, axis=0)\n",
    "    \n",
    "    # XGBoost模型预测\n",
    "    xgb_preds = []\n",
    "    for model in trained_models['xgb']:\n",
    "        xgb_preds.append(model.predict_proba(X_test_scaled_global)[:, 1])\n",
    "    test_meta_features[:, 1] = np.mean(xgb_preds, axis=0)\n",
    "    \n",
    "    # TabNet模型预测 (如果不跳过)\n",
    "    if not skip_tabnet:\n",
    "        tabnet_preds = []\n",
    "        for model in trained_models['tabnet']:\n",
    "            batch_size = 2048\n",
    "            num_test_samples = X_test_scaled_global.shape[0]\n",
    "            model_preds = []\n",
    "            \n",
    "            for i in range(0, num_test_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, num_test_samples)\n",
    "                batch_preds = model.predict_proba(X_test_scaled_global.iloc[i:end_idx].values)[:, 1]\n",
    "                model_preds.append(batch_preds)\n",
    "                \n",
    "            tabnet_preds.append(np.concatenate(model_preds))\n",
    "        \n",
    "        test_meta_features[:, 2] = np.mean(tabnet_preds, axis=0)\n",
    "    \n",
    "    # 继续其他处理,基模型的结果可能会有NaN\n",
    "    # 检查和填充 meta 特征中的 NaN\n",
    "    def check_and_fill_nan(features, feature_name):\n",
    "        \"\"\"增强版NaN检查和填充，可处理全NaN列\"\"\"\n",
    "\n",
    "        # 检查是否有NaN值\n",
    "        if np.isnan(features).any():\n",
    "            nan_count = np.isnan(features).sum()\n",
    "            logger.warning(f\"Found {nan_count} NaN values in {feature_name}.\")\n",
    "\n",
    "            # 检查并处理全NaN列\n",
    "            all_nan_cols = []\n",
    "            for col in range(features.shape[1]):\n",
    "                col_data = features[:, col]\n",
    "                if np.isnan(col_data).all():\n",
    "                    logger.warning(f\"Column {col} in {feature_name} is ALL NaN! Replacing with zeros.\")\n",
    "                    features[:, col] = 0\n",
    "                    all_nan_cols.append(col)\n",
    "                elif np.isnan(col_data).any():\n",
    "                    # 使用列均值填充部分NaN\n",
    "                    col_mean = np.nanmean(col_data)\n",
    "                    features[:, col] = np.nan_to_num(col_data, nan=col_mean)\n",
    "\n",
    "            if all_nan_cols:\n",
    "                logger.warning(f\"Replaced {len(all_nan_cols)} columns that were all NaN in {feature_name} with zeros\")\n",
    "\n",
    "            # 最后确保没有任何NaN残留\n",
    "            features = np.nan_to_num(features, nan=0.0)\n",
    "\n",
    "        # 断言确认没有NaN\n",
    "        assert not np.isnan(features).any(), f\"NaN values still exist in {feature_name} after imputation!\"\n",
    "\n",
    "        return features\n",
    "\n",
    "    # 检查和填充 meta 特征中的 NaN\n",
    "    cv_train_meta = check_and_fill_nan(cv_train_meta, \"cv_train_meta\")\n",
    "    test_meta_features = check_and_fill_nan(test_meta_features, \"test_meta_features\")\n",
    "    \n",
    "    # 修复3: 使用传入的验证集或从训练集划分\n",
    "    if X_val is not None and y_val is not None:\n",
    "        logger.info(\"Using provided validation set\")\n",
    "        # 使用提供的验证集\n",
    "        X_val_meta = np.zeros((X_val.shape[0], n_base_models))\n",
    "        \n",
    "        # 为验证集创建标准化器\n",
    "        val_scaler = StandardScaler()\n",
    "        X_val_scaled = pd.DataFrame(\n",
    "            val_scaler.fit_transform(X_val),\n",
    "            columns=X_val.columns,\n",
    "            index=X_val.index\n",
    "        )\n",
    "        \n",
    "        # 生成验证集的元特征\n",
    "        for fold in range(len(trained_models['lgbm'])):\n",
    "            # 获取对应fold的模型\n",
    "            lgbm_model = trained_models['lgbm'][fold]\n",
    "            xgb_model = trained_models['xgb'][fold]\n",
    "            \n",
    "            # LightGBM预测\n",
    "            lgbm_val_preds = lgbm_model.predict_proba(X_val_scaled)[:, 1]\n",
    "            \n",
    "            # XGBoost预测\n",
    "            xgb_val_preds = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "            \n",
    "            # 累加预测\n",
    "            X_val_meta[:, 0] += lgbm_val_preds\n",
    "            X_val_meta[:, 1] += xgb_val_preds\n",
    "            \n",
    "            # TabNet预测 (如果不跳过)\n",
    "            if not skip_tabnet:\n",
    "                tabnet_model = trained_models['tabnet'][fold]\n",
    "                batch_size = 2048\n",
    "                num_val_samples = X_val_scaled.shape[0]\n",
    "                tabnet_val_preds = []\n",
    "                \n",
    "                for i in range(0, num_val_samples, batch_size):\n",
    "                    end_idx = min(i + batch_size, num_val_samples)\n",
    "                    batch_preds = tabnet_model.predict_proba(X_val_scaled.iloc[i:end_idx].values)[:, 1]\n",
    "                    tabnet_val_preds.append(batch_preds)\n",
    "                \n",
    "                tabnet_val_preds = np.concatenate(tabnet_val_preds)\n",
    "                X_val_meta[:, 2] += tabnet_val_preds\n",
    "        \n",
    "        # 平均预测\n",
    "        X_val_meta /= len(trained_models['lgbm'])\n",
    "        \n",
    "        # 使用验证集标签\n",
    "        y_val_meta = y_val\n",
    "        \n",
    "        # 检查NaN\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_meta = check_and_fill_nan(X_val_meta, \"X_val_meta\")\n",
    "    else:\n",
    "        # 从训练集中划分出一部分作为元模型的验证集\n",
    "        X_train_meta, X_val_meta, y_train_meta, y_val_meta = train_test_split(\n",
    "            cv_train_meta, y_train, test_size=0.2, random_state=SEED, stratify=y_train\n",
    "        )\n",
    "    \n",
    "    # 修复4: 标准化元特征\n",
    "    meta_scaler = StandardScaler()\n",
    "    X_train_meta_scaled = meta_scaler.fit_transform(cv_train_meta)\n",
    "    X_val_meta_scaled = meta_scaler.transform(X_val_meta)\n",
    "    # 标准化后再次检查 NaN\n",
    "    X_train_meta_scaled = check_and_fill_nan(X_train_meta_scaled, \"X_train_meta_scaled\")\n",
    "    # 对测试集的元特征进行标准化\n",
    "    test_meta_features_scaled = meta_scaler.transform(test_meta_features)\n",
    "    test_meta_features_scaled = check_and_fill_nan(test_meta_features_scaled, \"test_meta_features_scaled (final)\")\n",
    "    \n",
    "    # 第四步：训练和优化元模型（逻辑回归）\n",
    "    logger.info(\"\\nTraining meta-model (Logistic Regression)...\")\n",
    "    \n",
    "    # 使用网格搜索优化元模型\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "    \n",
    "    # 样本量少时使用更强的正则化\n",
    "    if use_simplified_models:\n",
    "        C_values = [0.001, 0.01, 0.1, 1.0]  # 更强的正则化\n",
    "    else:\n",
    "        C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    \n",
    "    # 修复5: 考虑类不平衡问题，即使没有显式的class_weights也使用balanced权重\n",
    "    if class_weights:\n",
    "        meta_model = LogisticRegression(random_state=SEED, max_iter=2000, class_weight=class_weights, solver='liblinear')\n",
    "    else:\n",
    "        meta_model = LogisticRegression(random_state=SEED, max_iter=2000, class_weight='balanced', solver='liblinear')\n",
    "    \n",
    "    # 为小样本量减少参数网格规模\n",
    "    if use_simplified_models:\n",
    "        meta_param_grid = {'C': C_values, 'penalty': ['l2']}  # 只使用L2正则化\n",
    "    else:\n",
    "        meta_param_grid = {'C': C_values, 'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    # 小样本量时减少交叉验证折数以避免某些折中类别消失\n",
    "    cv_folds = 3 if use_simplified_models else 5\n",
    "    \n",
    "    # 修复6: 使用f1分数而不是AUC来优化元模型，更关注分类性能\n",
    "    grid_search = GridSearchCV(\n",
    "        meta_model, meta_param_grid, cv=cv_folds, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_meta_scaled, y_train)\n",
    "    \n",
    "    # 获取最佳元模型\n",
    "    best_meta_model = grid_search.best_estimator_\n",
    "    \n",
    "    # 在验证集上评估元模型\n",
    "    val_meta_preds = best_meta_model.predict_proba(X_val_meta_scaled)[:, 1]\n",
    "    val_meta_auc = roc_auc_score(y_val_meta, val_meta_preds)\n",
    "    \n",
    "    logger.info(f\"Meta-model validation AUC: {val_meta_auc:.4f}\")\n",
    "    logger.info(f\"Best meta-model parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # 使用全部训练数据重新训练元模型\n",
    "    final_meta_model = LogisticRegression(**grid_search.best_params_, random_state=SEED, max_iter=2000, solver='liblinear')\n",
    "    if class_weights:\n",
    "        final_meta_model.class_weight = class_weights\n",
    "    else:\n",
    "        final_meta_model.class_weight = 'balanced'  # 即使没有显式的class_weights也使用balanced\n",
    "    \n",
    "    final_meta_model.fit(X_train_meta_scaled, y_train)\n",
    "    \n",
    "    # 使用元模型进行最终预测\n",
    "    final_preds = final_meta_model.predict_proba(test_meta_features_scaled)[:, 1]\n",
    "    \n",
    "    # 计算最终的准确率\n",
    "    final_preds_binary = (final_preds >= 0.5).astype(int)\n",
    "    final_accuracy = accuracy_score(y_test, final_preds_binary)\n",
    "    logger.info(f\"Final test accuracy: {final_accuracy:.4f}\")\n",
    "    \n",
    "    # 评估模型\n",
    "    from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, classification_report\n",
    "    evaluation_results = plot_model_evaluation(y_test, final_preds, industry, \"Stacking\")\n",
    "    \n",
    "    # 修复7: 寻找最佳阈值 - 在验证集上寻找，而不是测试集\n",
    "    thresholds = np.arange(0.05, 1.0, 0.05)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    threshold_results = []\n",
    "    \n",
    "    # 在验证集上寻找最佳阈值\n",
    "    for threshold in thresholds:\n",
    "        y_val_pred_binary = (val_meta_preds >= threshold).astype(int)\n",
    "        \n",
    "        # 考虑不平衡数据，使用beta=2的F2分数，更注重召回率\n",
    "        precision = precision_score(y_val_meta, y_val_pred_binary, zero_division=0)\n",
    "        recall = recall_score(y_val_meta, y_val_pred_binary, zero_division=0)\n",
    "        \n",
    "        # 如果想更注重召回率，可以使用F2分数\n",
    "        beta = 2\n",
    "        f_beta = (1 + beta**2) * precision * recall / ((beta**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # 或者使用普通的F1分数\n",
    "        f1 = f1_score(y_val_meta, y_val_pred_binary, zero_division=0)\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'f1': f1,\n",
    "            'f_beta': f_beta,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        })\n",
    "        \n",
    "        # 选择使用哪种分数作为优化目标 (F1或F2)\n",
    "        if f1 > best_f1:  # 或者使用f_beta > best_f1\n",
    "            best_f1 = f1  # 记得同时更新变量\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    logger.info(f\"Best threshold: {best_threshold:.2f} (F1={best_f1:.4f})\")\n",
    "    \n",
    "    # 使用最佳阈值生成的预测\n",
    "    best_preds = (final_preds >= best_threshold).astype(int)\n",
    "    \n",
    "    # 为了更新最后的准确率报告\n",
    "    best_threshold_accuracy = accuracy_score(y_test, best_preds)\n",
    "    logger.info(f\"Final accuracy with best threshold ({best_threshold:.2f}): {best_threshold_accuracy:.4f}\")\n",
    "    \n",
    "    # 分析特征重要性 - 使用训练好的基础模型\n",
    "    if 'lgbm' in trained_models and len(trained_models['lgbm']) > 0 and 'xgb' in trained_models and len(trained_models['xgb']) > 0:\n",
    "        base_models_for_importance = {\n",
    "            'lgbm': trained_models['lgbm'][0],  # 使用第一个fold的模型\n",
    "            'xgb': trained_models['xgb'][0]\n",
    "        }\n",
    "        feature_importance = analyze_feature_importance(industry, X_train, y_train, base_models_for_importance)\n",
    "    else:\n",
    "        logger.warning(f\"Cannot analyze feature importance for {industry} - trained models are empty or incomplete\")\n",
    "        feature_importance = None\n",
    "\n",
    "    # 为了保存和部署，创建一个完整模型对象\n",
    "    complete_model = {\n",
    "        'industry': industry,\n",
    "        'meta_model': final_meta_model,\n",
    "        'lgbm_models': trained_models.get('lgbm', []),  # 使用get来避免键不存在的错误\n",
    "        'xgb_models': trained_models.get('xgb', []),\n",
    "        'tabnet_models': trained_models.get('tabnet', []) if not skip_tabnet else None,\n",
    "        'scalers': trained_scalers,  # 保存标准化器\n",
    "        'meta_scaler': meta_scaler,  # 保存元特征标准化器\n",
    "        'lgbm_params': lgbm_params,\n",
    "        'xgb_params': xgb_params,\n",
    "        'tabnet_params': tabnet_params if not skip_tabnet else None,\n",
    "        'features': list(X_train.columns),\n",
    "        'test_auc': evaluation_results['auc'],\n",
    "        'pr_auc': evaluation_results['pr_auc'],\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'threshold_results': threshold_results,\n",
    "        'class_weights': class_weights,\n",
    "        'train_date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'time_series': time_series,\n",
    "        'model_version': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        'use_simplified_models': use_simplified_models,\n",
    "        'skip_tabnet': skip_tabnet,\n",
    "        'n_base_models': n_base_models,\n",
    "        'early_stopped': early_stopping_triggered,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'best_threshold_accuracy': best_threshold_accuracy,\n",
    "        'target_accuracy': target_accuracy,\n",
    "        'accuracy_margin': accuracy_margin,\n",
    "        'early_stopped': early_stopping_triggered,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'best_threshold_accuracy': best_threshold_accuracy,\n",
    "        'target_accuracy': target_accuracy,\n",
    "        'accuracy_margin': accuracy_margin\n",
    "    }\n",
    "    \n",
    "    # 保存模型\n",
    "    save_model(complete_model, industry)\n",
    "    \n",
    "    return complete_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0ae12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d8d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a426c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unknown_samples(industry=None):\n",
    "    \"\"\"\n",
    "    对未知标签的数据进行预测，使用行业特定的最佳阈值，避免数据泄露\n",
    "    \n",
    "    Args:\n",
    "        industry: 如果指定，则只预测该行业的数据\n",
    "    \n",
    "    Returns:\n",
    "        预测结果DataFrame\n",
    "    \"\"\"\n",
    "    # 加载数据\n",
    "    logger.info(\"Loading data for prediction...\")\n",
    "    try:\n",
    "        financial_data = pd.read_csv('output/data_partial_balanced.csv')\n",
    "        industry_features_df = pd.read_csv('industry_features_results/all_industries_features.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Data file not found: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 创建特征字典\n",
    "    industry_feature_dict = {}\n",
    "    for ind in industry_features_df['Industry'].unique():\n",
    "        selected_features = industry_features_df[industry_features_df['Industry'] == ind]['Feature'].tolist()\n",
    "        industry_feature_dict[ind] = selected_features\n",
    "    \n",
    "    # 分离预测集\n",
    "    pred_data = financial_data[financial_data['FLAG'].isna()].copy()\n",
    "    pred_data = validate_and_clean_data(pred_data)  # 数据验证和清洗\n",
    "    \n",
    "    # 如果指定了行业，则只预测该行业\n",
    "    if industry:\n",
    "        pred_data = pred_data[pred_data['Industry'] == industry].copy()\n",
    "        logger.info(f\"Predicting for industry {industry} only. {len(pred_data)} samples to predict.\")\n",
    "    \n",
    "    # 获取所有可用的模型\n",
    "    model_files = []\n",
    "    for ext in ['pkl', 'joblib']:\n",
    "        model_files.extend([f for f in os.listdir('models') if f.endswith(f'_stacking_model.{ext}')])\n",
    "    \n",
    "    if len(model_files) == 0:\n",
    "        logger.error(\"No model files found. Please run models for industries first.\")\n",
    "        return None\n",
    "    \n",
    "    # 存储预测结果\n",
    "    pred_results = []\n",
    "    \n",
    "    # 按行业进行预测\n",
    "    for model_file in model_files:\n",
    "        # 提取行业名称 (支持多种文件扩展名)\n",
    "        model_industry = model_file.split('_stacking_model.')[0]\n",
    "        \n",
    "        # 如果指定了行业且不是当前模型的行业，则跳过\n",
    "        if industry and industry != model_industry:\n",
    "            continue\n",
    "        \n",
    "        # 避免重复处理同一行业的不同格式模型文件\n",
    "        if industry is None and any(r['Industry'] == model_industry for r in pred_results):\n",
    "            continue\n",
    "        \n",
    "        # 获取该行业的预测数据\n",
    "        industry_pred_data = pred_data[pred_data['Industry'] == model_industry].copy()\n",
    "        \n",
    "        if len(industry_pred_data) == 0:\n",
    "            logger.info(f\"No prediction data for industry {model_industry}\")\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"\\nPredicting for industry {model_industry} ({len(industry_pred_data)} samples)\")\n",
    "        \n",
    "        try:\n",
    "            # 加载模型 - 优先尝试joblib格式，因为它更适合大型模型\n",
    "            model_path = f'models/{model_industry}_stacking_model.joblib'\n",
    "            if not os.path.exists(model_path):\n",
    "                model_path = f'models/{model_industry}_stacking_model.pkl'\n",
    "            \n",
    "            if model_path.endswith('.joblib'):\n",
    "                model_results = joblib.load(model_path)\n",
    "            else:\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    model_results = pickle.load(f)\n",
    "            \n",
    "            # 获取该行业的特征\n",
    "            features = model_results['features']\n",
    "            \n",
    "            # 检查是否存在最佳阈值\n",
    "            threshold = model_results.get('best_threshold', 0.5)\n",
    "            logger.info(f\"Using threshold: {threshold} for industry {model_industry}\")\n",
    "            \n",
    "            # 检查是否是时间序列模型\n",
    "            if 'time_series' in model_results and model_results['time_series'] and model_results['date_col'] in industry_pred_data.columns:\n",
    "                # 按时间排序预测数据\n",
    "                industry_pred_data = industry_pred_data.sort_values(model_results['date_col'])\n",
    "                logger.info(f\"Sorted prediction data by {model_results['date_col']} for time series model\")\n",
    "            \n",
    "            # 确保所有特征都在数据集中\n",
    "            valid_features = [f for f in features if f in industry_pred_data.columns]\n",
    "            if len(valid_features) < len(features):\n",
    "                missing_features = set(features) - set(valid_features)\n",
    "                logger.warning(f\"Warning: {len(missing_features)} features not found for industry {model_industry}\")\n",
    "                logger.debug(f\"Missing features: {missing_features}\")\n",
    "                logger.info(f\"Using {len(valid_features)} valid features\")\n",
    "            \n",
    "            # 提取特征\n",
    "            X_pred = industry_pred_data[valid_features]\n",
    "            \n",
    "            # 修复1: 使用模型中保存的标准化器，避免重新拟合\n",
    "            # 初始化保存预测概率的数组\n",
    "            base_model_preds = np.zeros((X_pred.shape[0], 3))\n",
    "            \n",
    "            # 检查是否有保存的标准化器\n",
    "            if 'scalers' in model_results and len(model_results['scalers']) > 0:\n",
    "                # 使用第一个标准化器来处理预测数据\n",
    "                scaler = model_results['scalers'][0]\n",
    "                X_pred_scaled = pd.DataFrame(\n",
    "                    scaler.transform(X_pred),  # 使用transform而不是fit_transform\n",
    "                    columns=X_pred.columns,\n",
    "                    index=X_pred.index\n",
    "                )\n",
    "                logger.info(\"Using saved scaler for feature standardization\")\n",
    "            else:\n",
    "                # 如果没有保存的标准化器，则创建新的（应该避免这种情况）\n",
    "                logger.warning(\"No saved scaler found, creating a new one (not recommended)\")\n",
    "                scaler = StandardScaler()\n",
    "                X_pred_scaled = pd.DataFrame(\n",
    "                    scaler.fit_transform(X_pred),\n",
    "                    columns=X_pred.columns,\n",
    "                    index=X_pred.index\n",
    "                )\n",
    "            \n",
    "            # 获取元模型\n",
    "            meta_model = model_results['meta_model']\n",
    "            \n",
    "            # 修复2: 使用saved模型进行预测，跳过重新训练\n",
    "            if 'lgbm_models' in model_results and len(model_results['lgbm_models']) > 0:\n",
    "                # 使用保存的模型\n",
    "                logger.info(\"Using saved base models for prediction\")\n",
    "                \n",
    "                # 计算每个基础模型在每个fold上的预测，然后平均\n",
    "                lgbm_preds = []\n",
    "                xgb_preds = []\n",
    "                tabnet_preds = []\n",
    "                \n",
    "                # 获取fold数量\n",
    "                n_folds = len(model_results['lgbm_models'])\n",
    "                \n",
    "#                 for fold in range(n_folds):\n",
    "#                     # 选择对应fold的模型\n",
    "#                     lgbm_model = model_results['lgbm_models'][fold]\n",
    "#                     xgb_model = model_results['xgb_models'][fold]\n",
    "#                     tabnet_model = model_results['tabnet_models'][fold]\n",
    "                    \n",
    "#                     # 获取预测\n",
    "#                     lgbm_preds.append(lgbm_model.predict_proba(X_pred_scaled)[:, 1])\n",
    "#                     xgb_preds.append(xgb_model.predict_proba(X_pred_scaled)[:, 1])\n",
    "                    \n",
    "#                     # 分批处理TabNet预测以减少内存使用\n",
    "#                     batch_size = 2048\n",
    "#                     num_samples = X_pred_scaled.shape[0]\n",
    "#                     tabnet_pred_batch = []\n",
    "                    \n",
    "#                     for i in range(0, num_samples, batch_size):\n",
    "#                         end_idx = min(i + batch_size, num_samples)\n",
    "#                         batch = X_pred_scaled.iloc[i:end_idx].values\n",
    "#                         batch_pred = tabnet_model.predict_proba(batch)[:, 1]\n",
    "#                         tabnet_pred_batch.append(batch_pred)\n",
    "                    \n",
    "#                     tabnet_preds.append(np.concatenate(tabnet_pred_batch))\n",
    "                    \n",
    "#                     # 清理内存\n",
    "#                     gc.collect()\n",
    "#                     torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "                for fold in range(n_folds):\n",
    "                    # 选择对应fold的模型\n",
    "                    lgbm_model = model_results['lgbm_models'][fold]\n",
    "                    xgb_model = model_results['xgb_models'][fold]\n",
    "\n",
    "                    # 获取预测\n",
    "                    lgbm_preds.append(lgbm_model.predict_proba(X_pred_scaled)[:, 1])\n",
    "                    xgb_preds.append(xgb_model.predict_proba(X_pred_scaled)[:, 1])\n",
    "\n",
    "                    # 检查TabNet模型是否存在\n",
    "                    if 'tabnet_models' in model_results and model_results['tabnet_models'] is not None:\n",
    "                        # 确保TabNet模型在当前fold索引处存在\n",
    "                        if fold < len(model_results['tabnet_models']) and model_results['tabnet_models'][fold] is not None:\n",
    "                            # 分批处理TabNet预测以减少内存使用\n",
    "                            batch_size = 2048\n",
    "                            num_samples = X_pred_scaled.shape[0]\n",
    "                            tabnet_pred_batch = []\n",
    "\n",
    "                            for i in range(0, num_samples, batch_size):\n",
    "                                end_idx = min(i + batch_size, num_samples)\n",
    "                                batch = X_pred_scaled.iloc[i:end_idx].values\n",
    "                                batch_pred = model_results['tabnet_models'][fold].predict_proba(batch)[:, 1]\n",
    "                                tabnet_pred_batch.append(batch_pred)\n",
    "\n",
    "                            tabnet_preds.append(np.concatenate(tabnet_pred_batch))\n",
    "                        else:\n",
    "                            # TabNet模型在当前fold不存在，使用默认值(0.5)\n",
    "                            logger.warning(f\"TabNet model missing for fold {fold}. Using default probability of 0.5.\")\n",
    "                            tabnet_preds.append(np.ones(X_pred_scaled.shape[0]) * 0.5)\n",
    "                    else:\n",
    "                        # TabNet模型不存在，使用默认值(0.5)\n",
    "                        logger.warning(f\"No TabNet models found for industry {model_industry}. Using default probability of 0.5.\")\n",
    "                        tabnet_preds.append(np.ones(X_pred_scaled.shape[0]) * 0.5)\n",
    "\n",
    "                    # 清理内存\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    \n",
    "#                 # 平均每个fold的预测\n",
    "#                 base_model_preds[:, 0] = np.mean(lgbm_preds, axis=0)\n",
    "#                 base_model_preds[:, 1] = np.mean(xgb_preds, axis=0)\n",
    "#                 base_model_preds[:, 2] = np.mean(tabnet_preds, axis=0)\n",
    "                # 平均每个fold的预测\n",
    "                base_model_preds[:, 0] = np.mean(lgbm_preds, axis=0)\n",
    "                base_model_preds[:, 1] = np.mean(xgb_preds, axis=0)\n",
    "\n",
    "                # 检查是否有TabNet预测结果\n",
    "                if len(tabnet_preds) > 0:\n",
    "                    base_model_preds[:, 2] = np.mean(tabnet_preds, axis=0)\n",
    "                else:\n",
    "                    # 如果没有TabNet预测，使用默认值\n",
    "                    logger.warning(f\"No TabNet predictions available for industry {model_industry}. Using default probability of 0.5.\")\n",
    "                    base_model_preds[:, 2] = 0.5\n",
    "            else:\n",
    "                # 没有保存模型，需要重新训练（应该避免这种情况）\n",
    "                logger.warning(\"No saved base models found, retraining is not recommended\")\n",
    "                # ... [原始重新训练代码] ...\n",
    "                # 在这种情况下最好是中止操作，要求用户重新训练并保存正确的模型\n",
    "                logger.error(\"Retraining models during prediction is not supported in this version to avoid data leakage\")\n",
    "                return None\n",
    "            \n",
    "#             # 修复3: 使用保存的元特征标准化器\n",
    "#             if 'meta_scaler' in model_results:\n",
    "#                 # 标准化元特征\n",
    "#                 meta_scaler = model_results['meta_scaler']\n",
    "#                 base_model_preds_scaled = meta_scaler.transform(base_model_preds)\n",
    "#                 logger.info(\"Using saved meta-feature scaler\")\n",
    "#             else:\n",
    "#                 # 如果没有保存元特征标准化器，则直接使用（新模型应该都有这个组件）\n",
    "#                 logger.warning(\"No saved meta-feature scaler found, using raw predictions (not recommended)\")\n",
    "#                 base_model_preds_scaled = base_model_preds\n",
    "\n",
    "            if 'meta_scaler' in model_results:\n",
    "                # 标准化元特征\n",
    "                meta_scaler = model_results['meta_scaler']\n",
    "\n",
    "                # 检查元特征的数量是否匹配\n",
    "                # 获取meta_scaler期望的特征数量\n",
    "                expected_features = meta_scaler.n_features_in_ if hasattr(meta_scaler, 'n_features_in_') else meta_scaler.mean_.shape[0]\n",
    "                actual_features = base_model_preds.shape[1]\n",
    "\n",
    "                if expected_features == actual_features:\n",
    "                    # 如果特征数量匹配，直接使用\n",
    "                    base_model_preds_scaled = meta_scaler.transform(base_model_preds)\n",
    "                    logger.info(f\"Using saved meta-feature scaler with {expected_features} features\")\n",
    "                else:\n",
    "                    # 如果特征数量不匹配，则需要调整\n",
    "                    logger.warning(f\"Meta-scaler expects {expected_features} features, but got {actual_features} features\")\n",
    "\n",
    "                    if expected_features < actual_features:\n",
    "                        # 如果scaler期望的特征少于实际特征，只使用前几个特征\n",
    "                        # 例如，如果训练时只有LGBM和XGBoost，但预测时有LGBM、XGBoost和TabNet\n",
    "                        logger.warning(f\"Using only the first {expected_features} features for meta-scaler\")\n",
    "                        base_model_preds_scaled = meta_scaler.transform(base_model_preds[:, :expected_features])\n",
    "                    else:\n",
    "                        # 如果scaler期望的特征多于实际特征，需要填充缺失的特征\n",
    "                        # 这种情况比较罕见，但为了完整性也处理一下\n",
    "                        logger.warning(f\"Padding with zeros for missing {expected_features - actual_features} features\")\n",
    "                        padded_preds = np.zeros((base_model_preds.shape[0], expected_features))\n",
    "                        padded_preds[:, :actual_features] = base_model_preds\n",
    "                        base_model_preds_scaled = meta_scaler.transform(padded_preds)\n",
    "            else:\n",
    "                # 如果没有保存元特征标准化器，则直接使用（新模型应该都有这个组件）\n",
    "                logger.warning(\"No saved meta-feature scaler found, using raw predictions (not recommended)\")\n",
    "                base_model_preds_scaled = base_model_preds\n",
    "    \n",
    "            \n",
    "            # 使用元模型生成最终预测\n",
    "            logger.info(\"Generating final predictions with meta-model...\")\n",
    "            fraud_probs = meta_model.predict_proba(base_model_preds_scaled)[:, 1]\n",
    "            \n",
    "            # 使用行业特定的最佳阈值（如果存在）\n",
    "            fraud_preds = (fraud_probs >= threshold).astype(int)\n",
    "            \n",
    "            # 添加预测结果\n",
    "            for i, idx in enumerate(industry_pred_data.index):\n",
    "                pred_results.append({\n",
    "                    'ID': idx,\n",
    "                    'TICKER_SYMBOL': industry_pred_data.loc[idx, 'TICKER_SYMBOL'] if 'TICKER_SYMBOL' in industry_pred_data.columns else '',\n",
    "                    'Industry': model_industry,\n",
    "                    'Fraud_Probability': fraud_probs[i],\n",
    "                    'Predicted_Fraud': fraud_preds[i],\n",
    "                    'Threshold_Used': threshold\n",
    "                })\n",
    "            \n",
    "            # 清理内存\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting for industry {model_industry}: {e}\", exc_info=True)\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    if len(pred_results) > 0:\n",
    "        pred_results_df = pd.DataFrame(pred_results)\n",
    "        \n",
    "        # 保存预测结果\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if industry:\n",
    "            result_path = f'results/{industry}_predictions_{timestamp}.csv'\n",
    "        else:\n",
    "            result_path = f'results/all_predictions_{timestamp}.csv'\n",
    "        \n",
    "        pred_results_df.to_csv(result_path, index=False)\n",
    "        logger.info(f\"Predictions saved to {result_path}\")\n",
    "        \n",
    "        # 打印预测统计\n",
    "        logger.info(\"\\nPrediction statistics:\")\n",
    "        logger.info(f\"Total samples predicted: {len(pred_results_df)}\")\n",
    "        logger.info(f\"Predicted fraud cases: {pred_results_df['Predicted_Fraud'].sum()} ({pred_results_df['Predicted_Fraud'].mean():.2%})\")\n",
    "        \n",
    "        # 按行业统计\n",
    "        industry_stats = pred_results_df.groupby('Industry')[['Predicted_Fraud', 'Threshold_Used']].agg({\n",
    "            'Predicted_Fraud': ['count', 'sum', 'mean'],\n",
    "            'Threshold_Used': 'first'\n",
    "        })\n",
    "        industry_stats.columns = ['Count', 'Fraud_Count', 'Fraud_Ratio', 'Threshold']\n",
    "        industry_stats = industry_stats.sort_values('Fraud_Ratio', ascending=False)\n",
    "        \n",
    "        logger.info(\"\\nIndustry statistics (Top 5 by fraud ratio):\")\n",
    "        logger.info(f\"\\n{industry_stats.head(5)}\")\n",
    "        \n",
    "        # 创建预测可视化\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # 预测概率分布直方图\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.histplot(pred_results_df['Fraud_Probability'], bins=20)\n",
    "        plt.title('Distribution of Fraud Probabilities')\n",
    "        plt.xlabel('Probability')\n",
    "        plt.ylabel('Count')\n",
    "        # 显示汉字\n",
    "        plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        \n",
    "        # 按行业的预测比例条形图\n",
    "        plt.subplot(2, 2, 2)\n",
    "        \n",
    "        # 修复：检查是否只有一个行业，使用reset_index将索引转换为列\n",
    "        if len(industry_stats) > 0:  \n",
    "            # 先将索引转为列，这样不管是一个行业还是多个都能正确处理\n",
    "            plot_data = industry_stats.reset_index().sort_values('Count', ascending=False).head(10)\n",
    "            \n",
    "            if len(plot_data) > 0:\n",
    "                # 使用显式的列名而不是索引\n",
    "                sns.barplot(x='Fraud_Ratio', y='Industry', data=plot_data)\n",
    "                plt.title('Fraud Ratio by Industry')\n",
    "                plt.xlabel('Fraud Ratio')\n",
    "                plt.ylabel('Industry')\n",
    "                # 显示汉字\n",
    "                plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                plt.rcParams['axes.unicode_minus'] = False\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, \"No data to display\", \n",
    "                         horizontalalignment='center', verticalalignment='center')\n",
    "                plt.title('Fraud Ratio by Industry')\n",
    "                # 显示汉字\n",
    "                plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                plt.rcParams['axes.unicode_minus'] = False\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, \"No industry statistics available\", \n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "            plt.title('Fraud Ratio by Industry')\n",
    "            # 显示汉字\n",
    "            plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "            plt.rcParams['axes.unicode_minus'] = False\n",
    "        \n",
    "        # 添加阈值条形图\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if len(industry_stats) > 0:\n",
    "            plot_data = industry_stats.reset_index().sort_values('Threshold', ascending=False).head(10)\n",
    "            if len(plot_data) > 0:\n",
    "                sns.barplot(x='Threshold', y='Industry', data=plot_data)\n",
    "                plt.title('Threshold Used by Industry')\n",
    "                plt.xlabel('Threshold')\n",
    "                plt.ylabel('Industry')\n",
    "                # 显示汉字\n",
    "                plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                plt.rcParams['axes.unicode_minus'] = False\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, \"No data to display\", \n",
    "                         horizontalalignment='center', verticalalignment='center')\n",
    "                plt.title('Threshold Used by Industry')\n",
    "                # 显示汉字\n",
    "                plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                plt.rcParams['axes.unicode_minus'] = False\n",
    "                \n",
    "        # 显示汉字\n",
    "        plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        # 样本量与造假率散点图\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if len(industry_stats) > 0:\n",
    "            plt.scatter(\n",
    "                industry_stats['Count'], \n",
    "                industry_stats['Fraud_Ratio'], \n",
    "                alpha=0.7,\n",
    "                c=industry_stats['Threshold'],\n",
    "                cmap='viridis'\n",
    "            )\n",
    "            plt.colorbar(label='Threshold')\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Sample Count (log scale)')\n",
    "            plt.ylabel('Fraud Ratio')\n",
    "            plt.title('Sample Count vs Fraud Ratio')\n",
    "            # 显示汉字\n",
    "            plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "            plt.rcParams['axes.unicode_minus'] = False\n",
    "            \n",
    "            # 添加注释，标记出样本量最大的几个行业\n",
    "            for industry, row in industry_stats.head(5).iterrows():\n",
    "                plt.annotate(\n",
    "                    industry,\n",
    "                    (row['Count'], row['Fraud_Ratio']),\n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points'\n",
    "                )\n",
    "        # 显示汉字\n",
    "        plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        # 保存图表\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/predictions_summary_{timestamp}.png', dpi=600, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return pred_results_df\n",
    "    else:\n",
    "        logger.warning(\"No predictions were made.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43e46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_industry_results():\n",
    "    \"\"\"\n",
    "    合并所有行业的结果，生成汇总报告\n",
    "    \"\"\"\n",
    "    # 检查模型目录\n",
    "    logger.info(\"Combining industry results...\")\n",
    "    if not os.path.exists('models'):\n",
    "        logger.error(\"No models directory found. Please run models for industries first.\")\n",
    "        return\n",
    "    \n",
    "    # 获取所有模型文件\n",
    "    model_files = []\n",
    "    for ext in ['pkl', 'joblib']:\n",
    "        model_files.extend([f for f in os.listdir('models') if f.endswith(f'_stacking_model.{ext}')])\n",
    "    \n",
    "    # 去重（同一行业的不同格式）\n",
    "    unique_industries = set()\n",
    "    unique_model_files = []\n",
    "    for f in model_files:\n",
    "        industry = f.split('_stacking_model.')[0]\n",
    "        if industry not in unique_industries:\n",
    "            unique_industries.add(industry)\n",
    "            unique_model_files.append(f)\n",
    "    \n",
    "    if len(unique_model_files) == 0:\n",
    "        logger.error(\"No model files found. Please run models for industries first.\")\n",
    "        return\n",
    "    \n",
    "    # 加载所有模型结果\n",
    "    all_results = []\n",
    "    for model_file in unique_model_files:\n",
    "        try:\n",
    "            industry = model_file.split('_stacking_model.')[0]\n",
    "            model_path = os.path.join('models', model_file)\n",
    "            \n",
    "            if model_file.endswith('.joblib'):\n",
    "                result = joblib.load(model_path)\n",
    "            else:\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    result = pickle.load(f)\n",
    "            \n",
    "            all_results.append(result)\n",
    "            logger.info(f\"Loaded model for industry: {industry}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model {model_file}: {e}\", exc_info=True)\n",
    "    \n",
    "    # 生成汇总报告\n",
    "    summary = []\n",
    "    for result in all_results:\n",
    "        # 提取基本信息\n",
    "        summary.append({\n",
    "            'Industry': result['industry'],\n",
    "            'Test AUC': result['test_auc'],\n",
    "            'PR-AUC': result.get('pr_auc', 0),  # 兼容可能没有pr_auc的旧模型\n",
    "            'Features Count': len(result['features']),\n",
    "            'Training Date': result.get('train_date', 'Unknown')  # 兼容可能没有train_date的旧模型\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_df = summary_df.sort_values('Test AUC', ascending=False)\n",
    "    \n",
    "    logger.info(\"\\n=== Model Performance Summary ===\")\n",
    "    logger.info(f\"\\n{summary_df}\")\n",
    "    \n",
    "    # 保存结果\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    summary_df.to_csv(f'results/model_performance_summary_{timestamp}.csv', index=False)\n",
    "    \n",
    "    # 绘制性能对比图\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # AUC对比\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(x='Test AUC', y='Industry', data=summary_df.sort_values('Test AUC', ascending=False))\n",
    "    plt.title('AUC Score by Industry')\n",
    "    plt.xlim(0.5, 1.0)\n",
    "    plt.grid(True, axis='x')\n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # PR-AUC对比\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x='PR-AUC', y='Industry', data=summary_df.sort_values('PR-AUC', ascending=False))\n",
    "    plt.title('PR-AUC Score by Industry')\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.grid(True, axis='x')\n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/model_performance_comparison_{timestamp}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Summary saved to: results/model_performance_summary_{timestamp}.csv\")\n",
    "    logger.info(f\"Comparison chart saved to: plots/model_performance_comparison_{timestamp}.png\")\n",
    "    \n",
    "    # 模型性能统计\n",
    "    mean_auc = summary_df['Test AUC'].mean()\n",
    "    std_auc = summary_df['Test AUC'].std()\n",
    "    max_auc = summary_df['Test AUC'].max()\n",
    "    min_auc = summary_df['Test AUC'].min()\n",
    "    \n",
    "    logger.info(\"\\nPerformance statistics:\")\n",
    "    logger.info(f\"Average AUC: {mean_auc:.4f} (std: {std_auc:.4f})\")\n",
    "    logger.info(f\"Max AUC: {max_auc:.4f} (industry: {summary_df.loc[summary_df['Test AUC'].idxmax(), 'Industry']})\")\n",
    "    logger.info(f\"Min AUC: {min_auc:.4f} (industry: {summary_df.loc[summary_df['Test AUC'].idxmin(), 'Industry']})\")\n",
    "    \n",
    "    # 检查可能存在的数据泄露问题\n",
    "    suspicious_models = summary_df[summary_df['Test AUC'] > 0.95]\n",
    "    if len(suspicious_models) > 0:\n",
    "        logger.warning(\"\\nWARNING: The following models have suspiciously high AUC (>0.95), which might indicate data leakage:\")\n",
    "        for _, row in suspicious_models.iterrows():\n",
    "            logger.warning(f\"Industry: {row['Industry']}, AUC: {row['Test AUC']:.4f}\")\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f7d51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_industry(industry_name, time_series=False, date_col=None, target_accuracy=0.89, accuracy_margin=0.01):\n",
    "    \"\"\"\n",
    "    运行单个行业的模型\n",
    "    \n",
    "    Args:\n",
    "        industry_name: 行业名称\n",
    "        time_series: 是否按时间序列处理\n",
    "        date_col: 日期列名称，仅在time_series=True时使用\n",
    "        target_accuracy: 目标准确率，达到该准确率附近时停止训练\n",
    "        accuracy_margin: 准确率容差范围\n",
    "    \"\"\"\n",
    "    # 加载数据\n",
    "    try:\n",
    "        logger.info(f\"Loading data for industry: {industry_name}\")\n",
    "        financial_data = pd.read_csv('output/data_partial_balanced.csv')\n",
    "        industry_features_df = pd.read_csv('industry_features_results/all_industries_features.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 获取有监督特征选择的行业列表\n",
    "    supervised_industries = industry_features_df[industry_features_df['Method'] == '有监督']['Industry'].unique()\n",
    "    \n",
    "    # 确认行业是否在有监督列表中\n",
    "    if industry_name not in supervised_industries:\n",
    "        logger.error(f\"Error: Industry {industry_name} is not in the supervised industry list.\")\n",
    "        logger.info(f\"Available supervised industries: {supervised_industries}\")\n",
    "        return\n",
    "    \n",
    "    # 创建特征字典\n",
    "    industry_feature_dict = {}\n",
    "    for ind in industry_features_df['Industry'].unique():\n",
    "        selected_features = industry_features_df[industry_features_df['Industry'] == ind]['Feature'].tolist()\n",
    "        industry_feature_dict[ind] = selected_features\n",
    "    \n",
    "    # 分离预测集\n",
    "    train_data = financial_data[financial_data['FLAG'].notna()].copy()\n",
    "    \n",
    "    # 获取该行业的数据\n",
    "    industry_data = train_data[train_data['Industry'] == industry_name].copy()\n",
    "    \n",
    "    # 特殊处理小样本行业\n",
    "    use_simplified_models = False\n",
    "    skip_tabnet = False\n",
    "    n_folds = 5\n",
    "    \n",
    "    # 教育行业特殊处理\n",
    "    if industry_name == \"教育\":\n",
    "        logger.info(\"Education industry detected - using simplified models and increased regularization\")\n",
    "        use_simplified_models = True\n",
    "        skip_tabnet = True\n",
    "        n_folds = 3\n",
    "    \n",
    "    # 综合行业稍微简化\n",
    "    elif industry_name == \"综合\":\n",
    "        logger.info(\"Comprehensive industry detected - using moderately simplified models\")\n",
    "        use_simplified_models = True\n",
    "        n_folds = 4\n",
    "        \n",
    "    # 获取该行业的特征\n",
    "    industry_features = industry_feature_dict[industry_name]\n",
    "    \n",
    "    # 记录行业特征\n",
    "    logger.info(f\"Industry features: {len(industry_features)} features selected\")\n",
    "    \n",
    "    # 记录样本分布\n",
    "    fraud_count = industry_data['FLAG'].sum()\n",
    "    non_fraud_count = len(industry_data) - fraud_count\n",
    "    fraud_ratio = fraud_count / len(industry_data)\n",
    "    logger.info(f\"Sample distribution: {len(industry_data)} total, {fraud_count} fraud ({fraud_ratio:.2%}), {non_fraud_count} non-fraud\")\n",
    "    \n",
    "    # 记录目标准确率\n",
    "    logger.info(f\"Target accuracy: {target_accuracy:.4f} (±{accuracy_margin:.4f})\")\n",
    "    \n",
    "    # 处理单个行业，传递额外参数\n",
    "    result = process_single_industry(\n",
    "#         industry_name, \n",
    "#         industry_data, \n",
    "#         industry_features,\n",
    "#         time_series=time_series, \n",
    "#         date_col=date_col,\n",
    "#         use_simplified_models=use_simplified_models,\n",
    "#         skip_tabnet=skip_tabnet,\n",
    "#         n_folds=n_folds,\n",
    "#         target_accuracy=target_accuracy,\n",
    "#         accuracy_margin=accuracy_margin\n",
    "        industry_name, \n",
    "        industry_data, \n",
    "        industry_features,\n",
    "        time_series=time_series, \n",
    "        date_col=date_col,\n",
    "        use_simplified_models=use_simplified_models,\n",
    "        skip_tabnet=skip_tabnet,\n",
    "        n_folds=n_folds,\n",
    "        target_accuracy=target_accuracy,\n",
    "        accuracy_margin=accuracy_margin\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        logger.info(f\"Successfully processed industry: {industry_name}\")\n",
    "        logger.info(f\"Model saved to: models/{industry_name}_stacking_model.pkl and .joblib\")\n",
    "        logger.info(f\"Performance: AUC = {result['test_auc']:.4f}, PR-AUC = {result['pr_auc']:.4f}\")\n",
    "        \n",
    "        # 添加准确率相关信息\n",
    "        if 'final_accuracy' in result:\n",
    "            logger.info(f\"Final accuracy: {result['final_accuracy']:.4f}\")\n",
    "        if 'best_threshold_accuracy' in result:\n",
    "            logger.info(f\"Accuracy with best threshold: {result['best_threshold_accuracy']:.4f}\")\n",
    "        if 'early_stopped' in result and result['early_stopped']:\n",
    "            logger.info(f\"Model training was early stopped at target accuracy {target_accuracy:.4f}±{accuracy_margin:.4f}\")\n",
    "        \n",
    "        # 绘制该行业的性能摘要\n",
    "        performance_data = {\n",
    "            'Industry': [industry_name],\n",
    "            'AUC': [result['test_auc']],\n",
    "            'PR-AUC': [result['pr_auc']],\n",
    "            'Accuracy': [result.get('final_accuracy', 0)],\n",
    "            'Accuracy_Best_Threshold': [result.get('best_threshold_accuracy', 0)],\n",
    "            'Sample_Count': [len(industry_data)],\n",
    "            'Fraud_Ratio': [fraud_ratio],\n",
    "            'Early_Stopped': [result.get('early_stopped', False)]\n",
    "        }\n",
    "        pd.DataFrame(performance_data).to_csv(f'results/{industry_name}_performance.csv', index=False)\n",
    "    else:\n",
    "        logger.error(f\"Failed to process industry: {industry_name}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "337cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_evaluation(y_true, y_pred, industry, model_name=\"Stacking\"):\n",
    "    \"\"\"\n",
    "    绘制模型评估图表，包括ROC曲线、PR曲线、混淆矩阵等\n",
    "    \n",
    "    Args:\n",
    "        y_true: 真实标签\n",
    "        y_pred: 预测概率\n",
    "        industry: 行业名称\n",
    "        model_name: 模型名称\n",
    "    \"\"\"\n",
    "    # 计算评估指标\n",
    "    test_auc = roc_auc_score(y_true, y_pred)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # 转换为二分类预测\n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    \n",
    "    # 生成分类报告\n",
    "    report = classification_report(y_true, y_pred_binary, output_dict=True)\n",
    "    \n",
    "    # 创建评估图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # ROC曲线\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    axes[0, 0].plot(fpr, tpr, label=f'AUC = {test_auc:.4f}')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title(f'{industry} - {model_name} ROC Curve')\n",
    "    axes[0, 0].legend(loc='lower right')\n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # PR曲线\n",
    "    axes[0, 1].plot(recall, precision, label=f'PR-AUC = {pr_auc:.4f}')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title(f'{industry} - {model_name} Precision-Recall Curve')\n",
    "    axes[0, 1].legend(loc='lower left')\n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # 混淆矩阵\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('True')\n",
    "    axes[1, 0].set_title(f'{industry} - {model_name} Confusion Matrix')\n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # 分类指标 - 安全地获取类别键\n",
    "    # 检查报告中是否包含所需的类别\n",
    "    class_keys = []\n",
    "    for key in ['0', '1']:\n",
    "        if key in report:\n",
    "            class_keys.append(key)\n",
    "    \n",
    "    # 创建数据框\n",
    "    metrics_data = {}\n",
    "    \n",
    "    # 安全地获取每个指标\n",
    "    metrics_names = ['precision', 'recall', 'f1-score', 'support']\n",
    "    for metric in metrics_names:\n",
    "        metrics_data[metric.capitalize()] = []\n",
    "        for key in class_keys:\n",
    "            metrics_data[metric.capitalize()].append(report[key][metric])\n",
    "        \n",
    "        # 如果类别不全，补充缺失值\n",
    "        while len(metrics_data[metric.capitalize()]) < 2:\n",
    "            if metric == 'support':\n",
    "                metrics_data[metric.capitalize()].append(0)\n",
    "            else:\n",
    "                metrics_data[metric.capitalize()].append(float('nan'))\n",
    "    \n",
    "    # 创建索引名称\n",
    "    index_names = []\n",
    "    for i, key in enumerate(class_keys):\n",
    "        index_names.append(f'Class {key}')\n",
    "    \n",
    "    # 如果类别不全，补充缺失的类别名称\n",
    "    while len(index_names) < 2:\n",
    "        missing_class = '1' if '0' in class_keys else '0'\n",
    "        index_names.append(f'Class {missing_class} (Missing)')\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data, index=index_names)\n",
    "    \n",
    "    # 为分类指标表创建一个表格\n",
    "    table = axes[1, 1].table(\n",
    "        cellText=metrics_df.values.round(3),\n",
    "        rowLabels=metrics_df.index,\n",
    "        colLabels=metrics_df.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    table.scale(1, 1.5)\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].set_title(f'{industry} - {model_name} Classification Metrics')\n",
    "    \n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    # 保存图表\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{industry}_{model_name.lower()}_evaluation.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 记录评估结果\n",
    "    logger.info(f\"{industry} - {model_name} Evaluation:\")\n",
    "    logger.info(f\"AUC: {test_auc:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
    "    logger.info(f\"Classification Report:\\n{classification_report(y_true, y_pred_binary)}\")\n",
    "    \n",
    "    return {\n",
    "        'auc': test_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35a69428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(industry, X_train, y_train, trained_models=None):\n",
    "    \"\"\"\n",
    "    分析并可视化特征重要性，使用训练好的模型或训练新模型\n",
    "    \n",
    "    Args:\n",
    "        industry: 行业名称\n",
    "        X_train: 训练特征\n",
    "        y_train: 训练标签\n",
    "        trained_models: 已训练好的模型字典，包含 'lgbm' 和 'xgb' 键\n",
    "    \"\"\"\n",
    "    # 创建重要性数据框列表\n",
    "    importance_dfs = []\n",
    "    \n",
    "    # 如果没有提供训练好的模型，则训练新模型\n",
    "    if trained_models is None or 'lgbm' not in trained_models:\n",
    "        # LightGBM特征重要性\n",
    "        lgbm_model = lgb.LGBMClassifier(\n",
    "            objective='binary', \n",
    "            n_jobs=-1, \n",
    "            verbosity=-1, \n",
    "            random_state=SEED\n",
    "        )\n",
    "        lgbm_model.fit(X_train, y_train)\n",
    "    else:\n",
    "        lgbm_model = trained_models['lgbm']\n",
    "    \n",
    "    # 提取LightGBM特征重要性\n",
    "    lgbm_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': lgbm_model.feature_importances_,\n",
    "        'Model': 'LightGBM'\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    importance_dfs.append(lgbm_importance)\n",
    "    \n",
    "    # 如果没有提供训练好的模型，则训练新模型\n",
    "    if trained_models is None or 'xgb' not in trained_models:\n",
    "        # XGBoost特征重要性\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic', \n",
    "            eval_metric='auc', \n",
    "            use_label_encoder=False, \n",
    "            n_jobs=-1, \n",
    "            random_state=SEED\n",
    "        )\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "    else:\n",
    "        xgb_model = trained_models['xgb']\n",
    "    \n",
    "    # 提取XGBoost特征重要性\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': xgb_model.feature_importances_,\n",
    "        'Model': 'XGBoost'\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    importance_dfs.append(xgb_importance)\n",
    "    \n",
    "    # 合并重要性数据\n",
    "    all_importances = pd.concat(importance_dfs)\n",
    "    \n",
    "    # 绘制特征重要性\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # LightGBM特征重要性\n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_lgbm = lgbm_importance.head(15)\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_lgbm)\n",
    "    plt.title(f'{industry} - LightGBM Feature Importance')\n",
    "    \n",
    "    # XGBoost特征重要性\n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_xgb = xgb_importance.head(15)\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_xgb)\n",
    "    plt.title(f'{industry} - XGBoost Feature Importance')\n",
    "    \n",
    "    # 显示汉字\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{industry}_feature_importance.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 保存特征重要性数据\n",
    "    all_importances.to_csv(f'results/{industry}_feature_importance.csv', index=False)\n",
    "    \n",
    "    return all_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9229c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_industry_model(industry_name):\n",
    "    \"\"\"\n",
    "    详细评估单个行业的模型性能，计算最佳阈值并保存，避免数据泄露\n",
    "    \n",
    "    Args:\n",
    "        industry_name: 行业名称\n",
    "    \"\"\"\n",
    "    logger.info(f\"Evaluating model for industry: {industry_name}\")\n",
    "    \n",
    "    # 检查模型是否存在\n",
    "    model_path_joblib = f'models/{industry_name}_stacking_model.joblib'\n",
    "    model_path_pkl = f'models/{industry_name}_stacking_model.pkl'\n",
    "    \n",
    "    if os.path.exists(model_path_joblib):\n",
    "        model_path = model_path_joblib\n",
    "    elif os.path.exists(model_path_pkl):\n",
    "        model_path = model_path_pkl\n",
    "    else:\n",
    "        logger.error(f\"No model found for industry {industry_name}. Please train a model first.\")\n",
    "        return\n",
    "    \n",
    "    # 加载模型\n",
    "    logger.info(f\"Loading model from {model_path}\")\n",
    "    try:\n",
    "        if model_path.endswith('.joblib'):\n",
    "            model_results = joblib.load(model_path)\n",
    "        else:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_results = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\", exc_info=True)\n",
    "        return\n",
    "    \n",
    "    # 加载评估数据\n",
    "    try:\n",
    "        financial_data = pd.read_csv('output/data_partial_balanced.csv')\n",
    "        industry_features_df = pd.read_csv('industry_features_results/all_industries_features.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 获取行业特征\n",
    "    features = model_results['features']\n",
    "    \n",
    "    # 获取模型性能指标\n",
    "    test_auc = model_results.get('test_auc', None)\n",
    "    pr_auc = model_results.get('pr_auc', None)\n",
    "    train_date = model_results.get('train_date', 'Unknown')\n",
    "    \n",
    "    logger.info(f\"Model info: Trained on {train_date}\")\n",
    "    logger.info(f\"Performance metrics: AUC = {test_auc:.4f}, PR-AUC = {pr_auc:.4f} (from model)\")\n",
    "    \n",
    "    # 获取该行业的数据\n",
    "    industry_data = financial_data[financial_data['Industry'] == industry_name].copy()\n",
    "    labeled_data = industry_data[industry_data['FLAG'].notna()].copy()\n",
    "    unlabeled_data = industry_data[industry_data['FLAG'].isna()].copy()\n",
    "    \n",
    "    logger.info(f\"Data statistics: Total={len(industry_data)}, Labeled={len(labeled_data)}, Unlabeled={len(unlabeled_data)}\")\n",
    "    \n",
    "    # 验证有训练集结果\n",
    "    if 'meta_model' not in model_results:\n",
    "        logger.error(\"Invalid model format: missing meta_model\")\n",
    "        return\n",
    "    \n",
    "    # 创建评估报告目录\n",
    "    eval_dir = f'evaluations/{industry_name}'\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    # 创建模型信息报告\n",
    "    model_info = {\n",
    "        'Industry': industry_name,\n",
    "        'Train Date': train_date,\n",
    "        'Test AUC': test_auc,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'Features Count': len(features),\n",
    "        'Total Samples': len(industry_data),\n",
    "        'Labeled Samples': len(labeled_data),\n",
    "        'Unlabeled Samples': len(unlabeled_data)\n",
    "    }\n",
    "    \n",
    "    # 样本分布统计\n",
    "    if len(labeled_data) > 0:\n",
    "        fraud_count = labeled_data['FLAG'].sum()\n",
    "        non_fraud_count = len(labeled_data) - fraud_count\n",
    "        fraud_ratio = fraud_count / len(labeled_data)\n",
    "        \n",
    "        model_info.update({\n",
    "            'Fraud Count': fraud_count,\n",
    "            'Non-Fraud Count': non_fraud_count,\n",
    "            'Fraud Ratio': fraud_ratio\n",
    "        })\n",
    "    \n",
    "    pd.DataFrame([model_info]).to_csv(f'{eval_dir}/model_info.csv', index=False)\n",
    "    \n",
    "    # 特征重要性分析\n",
    "    if os.path.exists(f'results/{industry_name}_feature_importance.csv'):\n",
    "        feature_importance = pd.read_csv(f'results/{industry_name}_feature_importance.csv')\n",
    "        \n",
    "        # 特征重要性可视化\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, model_type in enumerate(['LightGBM', 'XGBoost']):\n",
    "            model_importance = feature_importance[feature_importance['Model'] == model_type]\n",
    "            top_features = model_importance.sort_values('Importance', ascending=False).head(20)\n",
    "            \n",
    "            plt.subplot(1, 2, i+1)\n",
    "            sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "            plt.title(f'{industry_name} - {model_type} Feature Importance')\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f'{eval_dir}/feature_importance.png', dpi=600, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 如果有标记数据，执行额外的评估\n",
    "    if len(labeled_data) > 0:\n",
    "        # 提取标记数据的特征和标签\n",
    "        valid_features = [f for f in features if f in labeled_data.columns]\n",
    "        if len(valid_features) < len(features):\n",
    "            logger.warning(f\"Missing {len(features) - len(valid_features)} features in labeled data\")\n",
    "        \n",
    "        X_eval = labeled_data[valid_features]\n",
    "        y_eval = labeled_data['FLAG']\n",
    "        \n",
    "        # 修复1: 使用模型中保存的标准化器，而不是重新拟合\n",
    "        base_model_preds = np.zeros((X_eval.shape[0], 3))\n",
    "        \n",
    "        # 检查是否有保存的标准化器\n",
    "        if 'scalers' in model_results and len(model_results['scalers']) > 0:\n",
    "            logger.info(\"Using saved scalers for evaluation\")\n",
    "            # 使用第一个保存的标准化器\n",
    "            scaler = model_results['scalers'][0]\n",
    "            X_eval_scaled = pd.DataFrame(\n",
    "                scaler.transform(X_eval),  # 使用transform而不是fit_transform\n",
    "                columns=X_eval.columns,\n",
    "                index=X_eval.index\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\"No saved scalers found in model, creating new scaler (not recommended)\")\n",
    "            # 其实这里不应该重新拟合，但如果没有保存的标准化器，只能重新做\n",
    "            scaler = StandardScaler()\n",
    "            X_eval_scaled = pd.DataFrame(\n",
    "                scaler.fit_transform(X_eval),\n",
    "                columns=X_eval.columns,\n",
    "                index=X_eval.index\n",
    "            )\n",
    "        \n",
    "        # 生成基础模型预测\n",
    "        if 'lgbm_models' in model_results and len(model_results['lgbm_models']) > 0:\n",
    "            # 使用保存的模型\n",
    "            logger.info(\"Using saved base models for evaluation\")\n",
    "            \n",
    "            # 计算每个基础模型的预测\n",
    "            lgbm_preds = []\n",
    "            xgb_preds = []\n",
    "            tabnet_preds = []\n",
    "            \n",
    "            # 获取fold数量\n",
    "            n_folds = len(model_results['lgbm_models'])\n",
    "            \n",
    "            for fold in range(n_folds):\n",
    "                lgbm_model = model_results['lgbm_models'][fold]\n",
    "                xgb_model = model_results['xgb_models'][fold]\n",
    "                tabnet_model = model_results['tabnet_models'][fold]\n",
    "                \n",
    "                lgbm_preds.append(lgbm_model.predict_proba(X_eval_scaled)[:, 1])\n",
    "                xgb_preds.append(xgb_model.predict_proba(X_eval_scaled)[:, 1])\n",
    "                \n",
    "                # 分批处理TabNet预测以减少内存使用\n",
    "                batch_size = 2048\n",
    "                num_samples = X_eval_scaled.shape[0]\n",
    "                tabnet_pred_batch = []\n",
    "                \n",
    "                for i in range(0, num_samples, batch_size):\n",
    "                    end_idx = min(i + batch_size, num_samples)\n",
    "                    batch = X_eval_scaled.iloc[i:end_idx].values\n",
    "                    batch_pred = tabnet_model.predict_proba(batch)[:, 1]\n",
    "                    tabnet_pred_batch.append(batch_pred)\n",
    "                \n",
    "                tabnet_preds.append(np.concatenate(tabnet_pred_batch))\n",
    "                \n",
    "                # 清理内存\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            # 平均每个fold的预测\n",
    "            base_model_preds[:, 0] = np.mean(lgbm_preds, axis=0)\n",
    "            base_model_preds[:, 1] = np.mean(xgb_preds, axis=0)\n",
    "            base_model_preds[:, 2] = np.mean(tabnet_preds, axis=0)\n",
    "        else:\n",
    "            logger.warning(\"No saved base models found, evaluation may be limited\")\n",
    "            # 在这种情况下不应尝试重新训练，因为这会引入数据泄露\n",
    "            logger.error(\"Cannot evaluate model without saved base models\")\n",
    "            return\n",
    "        \n",
    "        # 修复2: 使用保存的元特征标准化器\n",
    "        if 'meta_scaler' in model_results:\n",
    "            # 使用保存的元特征标准化器\n",
    "            meta_scaler = model_results['meta_scaler']\n",
    "            base_model_preds_scaled = meta_scaler.transform(base_model_preds)\n",
    "            logger.info(\"Using saved meta-feature scaler\")\n",
    "        else:\n",
    "            logger.warning(\"No saved meta-feature scaler found, using unscaled predictions\")\n",
    "            # 如果没有保存元特征标准化器，则使用未标准化的预测\n",
    "            base_model_preds_scaled = base_model_preds\n",
    "        \n",
    "        # 使用元模型进行最终预测\n",
    "        meta_model = model_results['meta_model']\n",
    "        final_preds = meta_model.predict_proba(base_model_preds_scaled)[:, 1]\n",
    "        \n",
    "        # 评估性能\n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "        from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "        \n",
    "        eval_results = plot_model_evaluation(y_eval, final_preds, industry_name, \"Evaluation\")\n",
    "        \n",
    "        # 保存评估图表\n",
    "        plt.savefig(f'{eval_dir}/model_evaluation.png', dpi=600, bbox_inches='tight')\n",
    "        \n",
    "        # 保存实际标签和预测概率\n",
    "        eval_df = pd.DataFrame({\n",
    "            'True_Label': y_eval,\n",
    "            'Predicted_Probability': final_preds,\n",
    "            'LightGBM_Prob': base_model_preds[:, 0],\n",
    "            'XGBoost_Prob': base_model_preds[:, 1],\n",
    "            'TabNet_Prob': base_model_preds[:, 2],\n",
    "            'Predicted_Label': (final_preds >= 0.5).astype(int)\n",
    "        })\n",
    "        \n",
    "        # 添加原始数据的索引\n",
    "        eval_df['Original_Index'] = labeled_data.index\n",
    "        \n",
    "        # 添加关键特征\n",
    "        for feature in features[:20]:  # 添加前20个特征\n",
    "            if feature in labeled_data.columns:\n",
    "                eval_df[feature] = labeled_data[feature].values\n",
    "        \n",
    "        # 保存预测结果\n",
    "        eval_df.to_csv(f'{eval_dir}/prediction_analysis.csv', index=False)\n",
    "        \n",
    "        # 修复3: 使用单独的验证集寻找最佳阈值\n",
    "        # 将评估数据分为两部分：一部分用于找最佳阈值，一部分用于最终评估\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # 对评估数据进行分层抽样\n",
    "        threshold_indices, final_indices = train_test_split(\n",
    "            np.arange(len(y_eval)), \n",
    "            test_size=0.5, \n",
    "            random_state=SEED, \n",
    "            stratify=y_eval\n",
    "        )\n",
    "        \n",
    "        # 分离数据\n",
    "        y_threshold = y_eval.iloc[threshold_indices]\n",
    "        y_final = y_eval.iloc[final_indices]\n",
    "        final_preds_threshold = final_preds[threshold_indices]\n",
    "        final_preds_final = final_preds[final_indices]\n",
    "        \n",
    "        # 在阈值划分数据上寻找最佳阈值\n",
    "        thresholds = np.arange(0.05, 1.0, 0.05)\n",
    "        threshold_metrics = []\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        # 寻找最佳阈值\n",
    "        for threshold in thresholds:\n",
    "            y_pred_binary = (final_preds_threshold >= threshold).astype(int)\n",
    "            \n",
    "            # 计算指标\n",
    "            accuracy = accuracy_score(y_threshold, y_pred_binary)\n",
    "            precision = precision_score(y_threshold, y_pred_binary, zero_division=0)\n",
    "            recall = recall_score(y_threshold, y_pred_binary, zero_division=0)\n",
    "            f1 = f1_score(y_threshold, y_pred_binary, zero_division=0)\n",
    "            \n",
    "            # 计算混淆矩阵元素\n",
    "            TP = np.sum((y_threshold == 1) & (y_pred_binary == 1))\n",
    "            FP = np.sum((y_threshold == 0) & (y_pred_binary == 1))\n",
    "            TN = np.sum((y_threshold == 0) & (y_pred_binary == 0))\n",
    "            FN = np.sum((y_threshold == 1) & (y_pred_binary == 0))\n",
    "            \n",
    "            # 对不平衡数据，可以使用F2分数，更重视召回率\n",
    "            beta = 2\n",
    "            f_beta = (1 + beta**2) * precision * recall / ((beta**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            threshold_metrics.append({\n",
    "                'Threshold': threshold,\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1_Score': f1,\n",
    "                'F2_Score': f_beta,\n",
    "                'TP': TP,\n",
    "                'FP': FP,\n",
    "                'TN': TN,\n",
    "                'FN': FN\n",
    "            })\n",
    "            \n",
    "            # 可以选择使用F1或F2分数作为选择标准\n",
    "            score_to_optimize = f1  # 或使用 f_beta\n",
    "            if score_to_optimize > best_f1:\n",
    "                best_f1 = score_to_optimize\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        # 保存阈值分析\n",
    "        threshold_df = pd.DataFrame(threshold_metrics)\n",
    "        threshold_df.to_csv(f'{eval_dir}/threshold_analysis.csv', index=False)\n",
    "        \n",
    "        logger.info(f\"Best threshold (on threshold subset): {best_threshold:.2f} (F1={best_f1:.4f})\")\n",
    "        \n",
    "        # 修复4: 在独立的最终评估集上用最佳阈值评估性能\n",
    "        y_final_pred_binary = (final_preds_final >= best_threshold).astype(int)\n",
    "        final_accuracy = accuracy_score(y_final, y_final_pred_binary)\n",
    "        final_precision = precision_score(y_final, y_final_pred_binary, zero_division=0)\n",
    "        final_recall = recall_score(y_final, y_final_pred_binary, zero_division=0)\n",
    "        final_f1 = f1_score(y_final, y_final_pred_binary, zero_division=0)\n",
    "        \n",
    "        logger.info(f\"Final evaluation (with best threshold):\")\n",
    "        logger.info(f\"Accuracy: {final_accuracy:.4f}, Precision: {final_precision:.4f}, Recall: {final_recall:.4f}, F1: {final_f1:.4f}\")\n",
    "        \n",
    "        # 绘制阈值性能曲线\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Accuracy'], marker='o')\n",
    "        plt.title('Accuracy vs Threshold')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.grid(True)\n",
    "        plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best Threshold ({best_threshold:.2f})')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Precision'], marker='o', label='Precision')\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Recall'], marker='s', label='Recall')\n",
    "        plt.title('Precision & Recall vs Threshold')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.axvline(x=best_threshold, color='r', linestyle='--')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['F1_Score'], marker='o')\n",
    "        plt.title('F1 Score vs Threshold')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.grid(True)\n",
    "        plt.axvline(x=best_threshold, color='r', linestyle='--')\n",
    "        \n",
    "        # 在最佳阈值处显示混淆矩阵\n",
    "        best_idx = threshold_df['Threshold'].values.tolist().index(best_threshold)\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.bar(['TP', 'FP', 'TN', 'FN'], \n",
    "                [threshold_df.iloc[best_idx]['TP'], \n",
    "                 threshold_df.iloc[best_idx]['FP'],\n",
    "                 threshold_df.iloc[best_idx]['TN'],\n",
    "                 threshold_df.iloc[best_idx]['FN']])\n",
    "        plt.title(f'Confusion Matrix at Best Threshold ({best_threshold:.2f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{eval_dir}/threshold_performance.png', dpi=600, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 将最佳阈值保存到模型文件中\n",
    "        if model_path.endswith('.joblib'):\n",
    "            model_results['best_threshold'] = best_threshold\n",
    "            model_results['best_f1'] = best_f1\n",
    "            model_results['evaluation_date'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            joblib.dump(model_results, model_path)\n",
    "            logger.info(f\"Updated model file with best threshold: {model_path}\")\n",
    "        else:\n",
    "            model_results['best_threshold'] = best_threshold\n",
    "            model_results['best_f1'] = best_f1\n",
    "            model_results['evaluation_date'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model_results, f)\n",
    "            logger.info(f\"Updated model file with best threshold: {model_path}\")\n",
    "        \n",
    "        # 执行错误分析\n",
    "        # 使用最佳阈值的预测结果\n",
    "        eval_df['Best_Threshold_Prediction'] = (final_preds >= best_threshold).astype(int)\n",
    "        eval_df['Error'] = (eval_df['Best_Threshold_Prediction'] != eval_df['True_Label']).astype(int)\n",
    "        \n",
    "        # 分析误判样本\n",
    "        false_positives = eval_df[(eval_df['True_Label'] == 0) & (eval_df['Best_Threshold_Prediction'] == 1)]\n",
    "        false_negatives = eval_df[(eval_df['True_Label'] == 1) & (eval_df['Best_Threshold_Prediction'] == 0)]\n",
    "        \n",
    "        logger.info(f\"Error analysis: False Positives={len(false_positives)}, False Negatives={len(false_negatives)}\")\n",
    "        \n",
    "        # 保存误判样本\n",
    "        if len(false_positives) > 0:\n",
    "            false_positives.to_csv(f'{eval_dir}/false_positives.csv', index=False)\n",
    "        if len(false_negatives) > 0:\n",
    "            false_negatives.to_csv(f'{eval_dir}/false_negatives.csv', index=False)\n",
    "    \n",
    "    # 对未标记数据执行预测分析\n",
    "    if len(unlabeled_data) > 0:\n",
    "        # 提取未标记数据的特征\n",
    "        valid_features = [f for f in features if f in unlabeled_data.columns]\n",
    "        X_pred = unlabeled_data[valid_features]\n",
    "        \n",
    "        # 修复5: 使用保存的标准化器对未标记数据进行标准化\n",
    "        if 'scalers' in model_results and len(model_results['scalers']) > 0:\n",
    "            # 使用第一个保存的标准化器\n",
    "            scaler = model_results['scalers'][0]\n",
    "            X_pred_scaled = pd.DataFrame(\n",
    "                scaler.transform(X_pred),  # 使用transform而不是fit_transform\n",
    "                columns=X_pred.columns,\n",
    "                index=X_pred.index\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\"No saved scalers found, using new scaler (not recommended)\")\n",
    "            scaler = StandardScaler()\n",
    "            X_pred_scaled = pd.DataFrame(\n",
    "                scaler.fit_transform(X_pred),\n",
    "                columns=X_pred.columns,\n",
    "                index=X_pred.index\n",
    "            )\n",
    "        \n",
    "        # 生成基础模型预测\n",
    "        base_model_preds = np.zeros((X_pred.shape[0], 3))\n",
    "        \n",
    "        if 'lgbm_models' in model_results and len(model_results['lgbm_models']) > 0:\n",
    "            # 使用保存的模型\n",
    "            logger.info(\"Predicting for unlabeled data\")\n",
    "            \n",
    "            # 计算每个基础模型的预测\n",
    "            lgbm_preds = []\n",
    "            xgb_preds = []\n",
    "            tabnet_preds = []\n",
    "            \n",
    "            # 获取fold数量\n",
    "            n_folds = len(model_results['lgbm_models'])\n",
    "            \n",
    "            for fold in range(n_folds):\n",
    "                lgbm_model = model_results['lgbm_models'][fold]\n",
    "                xgb_model = model_results['xgb_models'][fold]\n",
    "                tabnet_model = model_results['tabnet_models'][fold]\n",
    "                \n",
    "                lgbm_preds.append(lgbm_model.predict_proba(X_pred_scaled)[:, 1])\n",
    "                xgb_preds.append(xgb_model.predict_proba(X_pred_scaled)[:, 1])\n",
    "                \n",
    "                # 分批处理TabNet预测\n",
    "                batch_size = 2048\n",
    "                num_samples = X_pred_scaled.shape[0]\n",
    "                tabnet_pred_batch = []\n",
    "                \n",
    "                for i in range(0, num_samples, batch_size):\n",
    "                    end_idx = min(i + batch_size, num_samples)\n",
    "                    batch = X_pred_scaled.iloc[i:end_idx].values\n",
    "                    batch_pred = tabnet_model.predict_proba(batch)[:, 1]\n",
    "                    tabnet_pred_batch.append(batch_pred)\n",
    "                \n",
    "                tabnet_preds.append(np.concatenate(tabnet_pred_batch))\n",
    "                \n",
    "                # 清理内存\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            # 平均每个fold的预测\n",
    "            base_model_preds[:, 0] = np.mean(lgbm_preds, axis=0)\n",
    "            base_model_preds[:, 1] = np.mean(xgb_preds, axis=0)\n",
    "            base_model_preds[:, 2] = np.mean(tabnet_preds, axis=0)\n",
    "            \n",
    "            # 修复6: 使用保存的元特征标准化器\n",
    "            if 'meta_scaler' in model_results:\n",
    "                # 使用保存的元特征标准化器\n",
    "                meta_scaler = model_results['meta_scaler']\n",
    "                base_model_preds_scaled = meta_scaler.transform(base_model_preds)\n",
    "            else:\n",
    "                # 如果没有保存元特征标准化器，则使用未标准化的预测\n",
    "                base_model_preds_scaled = base_model_preds\n",
    "        \n",
    "            # 使用元模型进行最终预测\n",
    "            meta_model = model_results['meta_model']\n",
    "            final_preds = meta_model.predict_proba(base_model_preds_scaled)[:, 1]\n",
    "            \n",
    "            # 使用最佳阈值（如果有）\n",
    "            threshold = model_results.get('best_threshold', 0.5)\n",
    "            pred_labels = (final_preds >= threshold).astype(int)\n",
    "            \n",
    "            # 保存未标记数据的预测结果\n",
    "            unlabeled_pred_df = pd.DataFrame({\n",
    "                'Original_Index': unlabeled_data.index,\n",
    "                'Predicted_Probability': final_preds,\n",
    "                'Predicted_Label': pred_labels,\n",
    "                'Threshold_Used': threshold,\n",
    "                'LightGBM_Prob': base_model_preds[:, 0],\n",
    "                'XGBoost_Prob': base_model_preds[:, 1],\n",
    "                'TabNet_Prob': base_model_preds[:, 2]\n",
    "            })\n",
    "            \n",
    "            # 添加关键特征\n",
    "            for feature in features[:20]:  # 添加前20个特征\n",
    "                if feature in unlabeled_data.columns:\n",
    "                    unlabeled_pred_df[feature] = unlabeled_data[feature].values\n",
    "            \n",
    "            # 保存预测结果\n",
    "            unlabeled_pred_df.to_csv(f'{eval_dir}/unlabeled_predictions.csv', index=False)\n",
    "            \n",
    "            # 绘制预测分布\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(final_preds, bins=20)\n",
    "            plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.2f})')\n",
    "            plt.title('Prediction Probability Distribution (Unlabeled Data)')\n",
    "            plt.xlabel('Predicted Probability')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            fraud_count = sum(pred_labels)\n",
    "            non_fraud_count = len(pred_labels) - fraud_count\n",
    "            plt.pie([fraud_count, non_fraud_count], \n",
    "                    labels=['Predicted Fraud', 'Predicted Non-Fraud'], \n",
    "                    autopct='%1.1f%%')\n",
    "            plt.title('Prediction Distribution')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{eval_dir}/unlabeled_predictions_dist.png', dpi=600, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            logger.error(\"No saved models found, cannot predict for unlabeled data\")\n",
    "    \n",
    "    logger.info(f\"Evaluation for industry {industry_name} completed. Results saved to {eval_dir}/\")\n",
    "    if test_auc is not None:\n",
    "        logger.info(f\"AUC: {test_auc:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "    return f\"Evaluation completed for {industry_name}. Results saved to {eval_dir}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6aa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5869f3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:19:34,901 - __main__ - INFO - === 启动财务造假预测系统 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "  财务造假预测模型 - 分行业逐步运行优化版  \n",
      "============================================================\n",
      "\n",
      "当前目标准确率: 89.00% (±1.00%)\n",
      "\n",
      "请选择要执行的操作:\n",
      "1. 生成可用行业列表和统计分析\n",
      "2. 运行单个行业的模型\n",
      "3. 运行单个行业的模型 (时间序列处理)\n",
      "4. 合并所有行业的结果并生成汇总报告\n",
      "5. 对未知样本进行预测\n",
      "6. 对特定行业的未知样本进行预测\n",
      "7. 模型性能评估与可视化\n",
      "8. 数据探索性分析\n",
      "9. 单个行业模型详细评估\n",
      "10. 设置目标准确率和容差范围\n",
      "0. 退出\n",
      "\n",
      "请输入选项 (0-10): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:22:12,007 - __main__ - INFO - Loading data for prediction...\n",
      "2025-04-10 16:22:13,416 - __main__ - INFO - 发现缺失值: {'FLAG': 3905}\n",
      "2025-04-10 16:22:13,426 - __main__ - INFO - 特征'END_DATE_REP'发现49个异常值\n",
      "2025-04-10 16:22:13,435 - __main__ - INFO - 特征'CASH_C_EQUIV'发现186个异常值\n",
      "2025-04-10 16:22:13,440 - __main__ - INFO - 特征'NOTES_RECEIV'发现46个异常值\n",
      "2025-04-10 16:22:13,448 - __main__ - INFO - 特征'AR'发现45个异常值\n",
      "2025-04-10 16:22:13,455 - __main__ - INFO - 特征'PREPAYMENT'发现145个异常值\n",
      "2025-04-10 16:22:13,463 - __main__ - INFO - 特征'OTH_RECEIV'发现258个异常值\n",
      "2025-04-10 16:22:13,470 - __main__ - INFO - 特征'INVENTORIES'发现126个异常值\n",
      "2025-04-10 16:22:13,476 - __main__ - INFO - 特征'OTH_CA'发现92个异常值\n",
      "2025-04-10 16:22:13,483 - __main__ - INFO - 特征'T_CA'发现173个异常值\n",
      "2025-04-10 16:22:13,492 - __main__ - INFO - 特征'AVAIL_FOR_SALE_FA'发现301个异常值\n",
      "2025-04-10 16:22:13,500 - __main__ - INFO - 特征'LT_EQUITY_INVEST'发现273个异常值\n",
      "2025-04-10 16:22:13,506 - __main__ - INFO - 特征'INVEST_REAL_ESTATE'发现126个异常值\n",
      "2025-04-10 16:22:13,514 - __main__ - INFO - 特征'FIXED_ASSETS'发现153个异常值\n",
      "2025-04-10 16:22:13,521 - __main__ - INFO - 特征'CIP'发现138个异常值\n",
      "2025-04-10 16:22:13,530 - __main__ - INFO - 特征'INTAN_ASSETS'发现128个异常值\n",
      "2025-04-10 16:22:13,542 - __main__ - INFO - 特征'GOODWILL'发现297个异常值\n",
      "2025-04-10 16:22:13,551 - __main__ - INFO - 特征'LT_AMOR_EXP'发现138个异常值\n",
      "2025-04-10 16:22:13,561 - __main__ - INFO - 特征'DEFER_TAX_ASSETS'发现192个异常值\n",
      "2025-04-10 16:22:13,570 - __main__ - INFO - 特征'OTH_NCA'发现122个异常值\n",
      "2025-04-10 16:22:13,581 - __main__ - INFO - 特征'T_NCA'发现210个异常值\n",
      "2025-04-10 16:22:13,590 - __main__ - INFO - 特征'T_ASSETS'发现219个异常值\n",
      "2025-04-10 16:22:13,602 - __main__ - INFO - 特征'ST_BORR'发现134个异常值\n",
      "2025-04-10 16:22:13,611 - __main__ - INFO - 特征'NOTES_PAYABLE'发现74个异常值\n",
      "2025-04-10 16:22:13,625 - __main__ - INFO - 特征'AP'发现164个异常值\n",
      "2025-04-10 16:22:13,637 - __main__ - INFO - 特征'ADVANCE_RECEIPTS'发现209个异常值\n",
      "2025-04-10 16:22:13,650 - __main__ - INFO - 特征'PAYROLL_PAYABLE'发现119个异常值\n",
      "2025-04-10 16:22:13,663 - __main__ - INFO - 特征'TAXES_PAYABLE'发现181个异常值\n",
      "2025-04-10 16:22:13,673 - __main__ - INFO - 特征'INT_PAYABLE'发现374个异常值\n",
      "2025-04-10 16:22:13,686 - __main__ - INFO - 特征'OTH_PAYABLE'发现255个异常值\n",
      "2025-04-10 16:22:13,698 - __main__ - INFO - 特征'NCL_WITHIN_1Y'发现339个异常值\n",
      "2025-04-10 16:22:13,708 - __main__ - INFO - 特征'T_CL'发现238个异常值\n",
      "2025-04-10 16:22:13,722 - __main__ - INFO - 特征'LT_BORR'发现305个异常值\n",
      "2025-04-10 16:22:13,734 - __main__ - INFO - 特征'DEFER_REVENUE'发现74个异常值\n",
      "2025-04-10 16:22:13,746 - __main__ - INFO - 特征'DEFER_TAX_LIAB'发现274个异常值\n",
      "2025-04-10 16:22:13,759 - __main__ - INFO - 特征'T_NCL'发现287个异常值\n",
      "2025-04-10 16:22:13,770 - __main__ - INFO - 特征'T_LIAB'发现258个异常值\n",
      "2025-04-10 16:22:13,781 - __main__ - INFO - 特征'PAID_IN_CAPITAL'发现118个异常值\n",
      "2025-04-10 16:22:13,791 - __main__ - INFO - 特征'CAPITAL_RESER'发现112个异常值\n",
      "2025-04-10 16:22:13,801 - __main__ - INFO - 特征'SURPLUS_RESER'发现177个异常值\n",
      "2025-04-10 16:22:13,815 - __main__ - INFO - 特征'RETAINED_EARNINGS'发现158个异常值\n",
      "2025-04-10 16:22:13,825 - __main__ - INFO - 特征'T_EQUITY_ATTR_P'发现155个异常值\n",
      "2025-04-10 16:22:13,836 - __main__ - INFO - 特征'MINORITY_INT'发现208个异常值\n",
      "2025-04-10 16:22:13,844 - __main__ - INFO - 特征'T_SH_EQUITY'发现160个异常值\n",
      "2025-04-10 16:22:13,856 - __main__ - INFO - 特征'T_LIAB_EQUITY'发现219个异常值\n",
      "2025-04-10 16:22:13,868 - __main__ - INFO - 特征'OTH_COMPRE_INCOME'发现678个异常值\n",
      "2025-04-10 16:22:13,880 - __main__ - INFO - 特征'C_PAID_OTH_FINAN_A'发现96个异常值\n",
      "2025-04-10 16:22:13,892 - __main__ - INFO - 特征'N_CF_FR_INVEST_A'发现159个异常值\n",
      "2025-04-10 16:22:13,902 - __main__ - INFO - 特征'C_FR_BORR'发现190个异常值\n",
      "2025-04-10 16:22:13,913 - __main__ - INFO - 特征'N_CF_OPERATE_A'发现232个异常值\n",
      "2025-04-10 16:22:13,923 - __main__ - INFO - 特征'C_FR_CAP_CONTR'发现230个异常值\n",
      "2025-04-10 16:22:13,932 - __main__ - INFO - 特征'C_PAID_INVEST'发现102个异常值\n",
      "2025-04-10 16:22:13,943 - __main__ - INFO - 特征'C_FR_OTH_FINAN_A'发现162个异常值\n",
      "2025-04-10 16:22:13,955 - __main__ - INFO - 特征'C_PAID_OTH_INVEST_A'发现322个异常值\n",
      "2025-04-10 16:22:13,964 - __main__ - INFO - 特征'C_INF_FR_INVEST_A'发现81个异常值\n",
      "2025-04-10 16:22:13,973 - __main__ - INFO - 特征'C_PAID_G_S'发现202个异常值\n",
      "2025-04-10 16:22:13,985 - __main__ - INFO - 特征'C_PAID_FOR_OTH_OP_A'发现151个异常值\n",
      "2025-04-10 16:22:13,998 - __main__ - INFO - 特征'PROC_SELL_INVEST'发现72个异常值\n",
      "2025-04-10 16:22:14,011 - __main__ - INFO - 特征'C_FR_SALE_G_S'发现178个异常值\n",
      "2025-04-10 16:22:14,024 - __main__ - INFO - 特征'REFUND_OF_TAX'发现28个异常值\n",
      "2025-04-10 16:22:14,036 - __main__ - INFO - 特征'C_INF_FR_OPERATE_A'发现206个异常值\n",
      "2025-04-10 16:22:14,046 - __main__ - INFO - 特征'GAIN_INVEST'发现200个异常值\n",
      "2025-04-10 16:22:14,056 - __main__ - INFO - 特征'N_CHANGE_IN_CASH'发现176个异常值\n",
      "2025-04-10 16:22:14,068 - __main__ - INFO - 特征'FOREX_EFFECTS'发现98个异常值\n",
      "2025-04-10 16:22:14,084 - __main__ - INFO - 特征'C_PAID_FOR_TAXES'发现202个异常值\n",
      "2025-04-10 16:22:14,096 - __main__ - INFO - 特征'C_PAID_DIV_PROF_INT'发现187个异常值\n",
      "2025-04-10 16:22:14,106 - __main__ - INFO - 特征'C_FR_OTH_INVEST_A'发现117个异常值\n",
      "2025-04-10 16:22:14,120 - __main__ - INFO - 特征'C_OUTF_OPERATE_A'发现188个异常值\n",
      "2025-04-10 16:22:14,134 - __main__ - INFO - 特征'C_FR_OTH_OPERATE_A'发现216个异常值\n",
      "2025-04-10 16:22:14,146 - __main__ - INFO - 特征'C_OUTF_FR_INVEST_A'发现139个异常值\n",
      "2025-04-10 16:22:14,163 - __main__ - INFO - 特征'C_PAID_FOR_DEBTS'发现244个异常值\n",
      "2025-04-10 16:22:14,179 - __main__ - INFO - 特征'DISP_FIX_ASSETS_OTH'发现165个异常值\n",
      "2025-04-10 16:22:14,196 - __main__ - INFO - 特征'N_CF_FR_FINAN_A'发现278个异常值\n",
      "2025-04-10 16:22:14,209 - __main__ - INFO - 特征'N_CE_BEG_BAL'发现196个异常值\n",
      "2025-04-10 16:22:14,219 - __main__ - INFO - 特征'N_CE_END_BAL'发现195个异常值\n",
      "2025-04-10 16:22:14,228 - __main__ - INFO - 特征'PUR_FIX_ASSETS_OTH'发现120个异常值\n",
      "2025-04-10 16:22:14,239 - __main__ - INFO - 特征'C_INF_FR_FINAN_A'发现241个异常值\n",
      "2025-04-10 16:22:14,248 - __main__ - INFO - 特征'C_PAID_TO_FOR_EMPL'发现91个异常值\n",
      "2025-04-10 16:22:14,256 - __main__ - INFO - 特征'C_OUTF_FR_FINAN_A'发现238个异常值\n",
      "2025-04-10 16:22:14,268 - __main__ - INFO - 特征'ADMIN_EXP'发现82个异常值\n",
      "2025-04-10 16:22:14,284 - __main__ - INFO - 特征'A_J_INVEST_INCOME'发现655个异常值\n",
      "2025-04-10 16:22:14,292 - __main__ - INFO - 特征'REVENUE'发现172个异常值\n",
      "2025-04-10 16:22:14,306 - __main__ - INFO - 特征'T_REVENUE'发现173个异常值\n",
      "2025-04-10 16:22:14,316 - __main__ - INFO - 特征'T_PROFIT'发现149个异常值\n",
      "2025-04-10 16:22:14,331 - __main__ - INFO - 特征'OPERATE_PROFIT'发现157个异常值\n",
      "2025-04-10 16:22:14,340 - __main__ - INFO - 特征'ASSETS_IMPAIR_LOSS'发现88个异常值\n",
      "2025-04-10 16:22:14,352 - __main__ - INFO - 特征'COMPR_INC_ATTR_P'发现145个异常值\n",
      "2025-04-10 16:22:14,362 - __main__ - INFO - 特征'OTH_COMPR_INCOME'发现641个异常值\n",
      "2025-04-10 16:22:14,371 - __main__ - INFO - 特征'T_COMPR_INCOME'发现148个异常值\n",
      "2025-04-10 16:22:14,381 - __main__ - INFO - 特征'N_INCOME_ATTR_P'发现144个异常值\n",
      "2025-04-10 16:22:14,389 - __main__ - INFO - 特征'INVEST_INCOME'发现267个异常值\n",
      "2025-04-10 16:22:14,399 - __main__ - INFO - 特征'ASSETS_DISP_GAIN'发现702个异常值\n",
      "2025-04-10 16:22:14,410 - __main__ - INFO - 特征'FINAN_EXP'发现223个异常值\n",
      "2025-04-10 16:22:14,422 - __main__ - INFO - 特征'SELL_EXP'发现80个异常值\n",
      "2025-04-10 16:22:14,431 - __main__ - INFO - 特征'COGS'发现179个异常值\n",
      "2025-04-10 16:22:14,441 - __main__ - INFO - 特征'N_INCOME'发现146个异常值\n",
      "2025-04-10 16:22:14,450 - __main__ - INFO - 特征'BIZ_TAX_SURCHG'发现228个异常值\n",
      "2025-04-10 16:22:14,461 - __main__ - INFO - 特征'NOPERATE_EXP'发现184个异常值\n",
      "2025-04-10 16:22:14,471 - __main__ - INFO - 特征'GOING_CONCERN_NI'发现119个异常值\n",
      "2025-04-10 16:22:14,479 - __main__ - INFO - 特征'COMPR_INC_ATTR_M_S'发现284个异常值\n",
      "2025-04-10 16:22:14,490 - __main__ - INFO - 特征'INCOME_TAX'发现248个异常值\n",
      "2025-04-10 16:22:14,502 - __main__ - INFO - 特征'NOPERATE_INCOME'发现377个异常值\n",
      "2025-04-10 16:22:14,511 - __main__ - INFO - 特征'MINORITY_GAIN'发现288个异常值\n",
      "2025-04-10 16:22:14,521 - __main__ - INFO - 特征'T_COGS'发现175个异常值\n",
      "2025-04-10 16:22:14,621 - __main__ - INFO - \n",
      "Predicting for industry 交通运输、仓储和邮政业 (102 samples)\n",
      "2025-04-10 16:22:14,798 - __main__ - INFO - Using threshold: 0.55 for industry 交通运输、仓储和邮政业\n",
      "2025-04-10 16:22:14,806 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:14,807 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:15,276 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:15,279 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:15,475 - __main__ - INFO - \n",
      "Predicting for industry 信息传输、软件和信息技术服务业 (325 samples)\n",
      "2025-04-10 16:22:15,749 - __main__ - INFO - Using threshold: 0.9500000000000001 for industry 信息传输、软件和信息技术服务业\n",
      "2025-04-10 16:22:15,754 - __main__ - INFO - Using saved scaler for feature standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:22:15,754 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:16,174 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:16,175 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:16,366 - __main__ - INFO - \n",
      "Predicting for industry 农、林、牧、渔业 (41 samples)\n",
      "2025-04-10 16:22:16,650 - __main__ - INFO - Using threshold: 0.15000000000000002 for industry 农、林、牧、渔业\n",
      "2025-04-10 16:22:16,655 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:16,656 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:16,870 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:16,872 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:17,055 - __main__ - INFO - \n",
      "Predicting for industry 制造业 (2500 samples)\n",
      "2025-04-10 16:22:17,245 - __main__ - INFO - Using threshold: 0.7000000000000001 for industry 制造业\n",
      "2025-04-10 16:22:17,250 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:17,251 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:18,495 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:18,496 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:18,722 - __main__ - INFO - \n",
      "Predicting for industry 建筑业 (87 samples)\n",
      "2025-04-10 16:22:19,086 - __main__ - INFO - Using threshold: 0.5 for industry 建筑业\n",
      "2025-04-10 16:22:19,090 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:19,092 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:19,338 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:19,340 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:19,533 - __main__ - INFO - \n",
      "Predicting for industry 房地产业 (108 samples)\n",
      "2025-04-10 16:22:19,888 - __main__ - INFO - Using threshold: 0.9500000000000001 for industry 房地产业\n",
      "2025-04-10 16:22:19,888 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:19,888 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:20,206 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:20,209 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:20,393 - __main__ - INFO - \n",
      "Predicting for industry 批发和零售业 (162 samples)\n",
      "2025-04-10 16:22:20,618 - __main__ - INFO - Using threshold: 0.6000000000000001 for industry 批发和零售业\n",
      "2025-04-10 16:22:20,622 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:20,623 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:20,896 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:20,899 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:21,106 - __main__ - INFO - \n",
      "Predicting for industry 教育 (9 samples)\n",
      "2025-04-10 16:22:21,156 - __main__ - INFO - Using threshold: 0.45 for industry 教育\n",
      "2025-04-10 16:22:21,161 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:21,162 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:21,176 - __main__ - WARNING - No TabNet models found for industry 教育. Using default probability of 0.5.\n",
      "2025-04-10 16:22:21,323 - __main__ - WARNING - No TabNet models found for industry 教育. Using default probability of 0.5.\n",
      "2025-04-10 16:22:21,486 - __main__ - WARNING - No TabNet models found for industry 教育. Using default probability of 0.5.\n",
      "2025-04-10 16:22:21,698 - __main__ - WARNING - No TabNet models found for industry 教育. Using default probability of 0.5.\n",
      "2025-04-10 16:22:21,899 - __main__ - WARNING - No TabNet models found for industry 教育. Using default probability of 0.5.\n",
      "2025-04-10 16:22:22,063 - __main__ - WARNING - Meta-scaler expects 2 features, but got 3 features\n",
      "2025-04-10 16:22:22,065 - __main__ - WARNING - Using only the first 2 features for meta-scaler\n",
      "2025-04-10 16:22:22,067 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:22,262 - __main__ - INFO - \n",
      "Predicting for industry 文化、体育和娱乐业 (57 samples)\n",
      "2025-04-10 16:22:22,624 - __main__ - INFO - Using threshold: 0.9000000000000001 for industry 文化、体育和娱乐业\n",
      "2025-04-10 16:22:22,628 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:22,628 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:22,861 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:22,861 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:23,050 - __main__ - INFO - \n",
      "Predicting for industry 水利、环境和公共设施管理业 (68 samples)\n",
      "2025-04-10 16:22:23,226 - __main__ - INFO - Using threshold: 0.4 for industry 水利、环境和公共设施管理业\n",
      "2025-04-10 16:22:23,229 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:23,231 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:23,416 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:23,418 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:23,629 - __main__ - INFO - \n",
      "Predicting for industry 电力、热力、燃气及水生产和供应业 (110 samples)\n",
      "2025-04-10 16:22:23,960 - __main__ - INFO - Using threshold: 0.05 for industry 电力、热力、燃气及水生产和供应业\n",
      "2025-04-10 16:22:23,964 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:23,966 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:24,241 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:24,243 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:24,451 - __main__ - INFO - \n",
      "Predicting for industry 科学研究和技术服务业 (59 samples)\n",
      "2025-04-10 16:22:24,733 - __main__ - INFO - Using threshold: 0.05 for industry 科学研究和技术服务业\n",
      "2025-04-10 16:22:24,737 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:24,738 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:25,019 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:25,022 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:25,201 - __main__ - INFO - \n",
      "Predicting for industry 综合 (16 samples)\n",
      "2025-04-10 16:22:25,545 - __main__ - INFO - Using threshold: 0.45 for industry 综合\n",
      "2025-04-10 16:22:25,551 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:25,553 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:25,813 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:25,814 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:26,011 - __main__ - INFO - \n",
      "Predicting for industry 采矿业 (74 samples)\n",
      "2025-04-10 16:22:26,256 - __main__ - INFO - Using threshold: 0.9500000000000001 for industry 采矿业\n",
      "2025-04-10 16:22:26,261 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:26,262 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:26,485 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:26,486 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:26,697 - __main__ - INFO - \n",
      "Predicting for industry 金融业 (110 samples)\n",
      "2025-04-10 16:22:26,884 - __main__ - INFO - Using threshold: 0.55 for industry 金融业\n",
      "2025-04-10 16:22:26,887 - __main__ - INFO - Using saved scaler for feature standardization\n",
      "2025-04-10 16:22:26,887 - __main__ - INFO - Using saved base models for prediction\n",
      "2025-04-10 16:22:27,092 - __main__ - INFO - Using saved meta-feature scaler with 3 features\n",
      "2025-04-10 16:22:27,094 - __main__ - INFO - Generating final predictions with meta-model...\n",
      "2025-04-10 16:22:27,374 - __main__ - INFO - Predictions saved to results/all_predictions_20250410_162227.csv\n",
      "2025-04-10 16:22:27,375 - __main__ - INFO - \n",
      "Prediction statistics:\n",
      "2025-04-10 16:22:27,376 - __main__ - INFO - Total samples predicted: 3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:22:27,380 - __main__ - INFO - Predicted fraud cases: 64 (1.67%)\n",
      "2025-04-10 16:22:27,392 - __main__ - INFO - \n",
      "Industry statistics (Top 5 by fraud ratio):\n",
      "2025-04-10 16:22:27,398 - __main__ - INFO - \n",
      "               Count  Fraud_Count  Fraud_Ratio  Threshold\n",
      "Industry                                                 \n",
      "教育                 9            2     0.222222       0.45\n",
      "文化、体育和娱乐业         57           12     0.210526       0.90\n",
      "水利、环境和公共设施管理业     68            8     0.117647       0.40\n",
      "农、林、牧、渔业          41            3     0.073171       0.15\n",
      "金融业              110            8     0.072727       0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "  财务造假预测模型 - 分行业逐步运行优化版  \n",
      "============================================================\n",
      "\n",
      "当前目标准确率: 89.00% (±1.00%)\n",
      "\n",
      "请选择要执行的操作:\n",
      "1. 生成可用行业列表和统计分析\n",
      "2. 运行单个行业的模型\n",
      "3. 运行单个行业的模型 (时间序列处理)\n",
      "4. 合并所有行业的结果并生成汇总报告\n",
      "5. 对未知样本进行预测\n",
      "6. 对特定行业的未知样本进行预测\n",
      "7. 模型性能评估与可视化\n",
      "8. 数据探索性分析\n",
      "9. 单个行业模型详细评估\n",
      "10. 设置目标准确率和容差范围\n",
      "0. 退出\n",
      "\n",
      "请输入选项 (0-10): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:22:36,615 - __main__ - INFO - \n",
      "程序结束，感谢使用！\n"
     ]
    }
   ],
   "source": [
    "def main_menu():\n",
    "    \"\"\"\n",
    "    主菜单，提供不同的功能选项，增加了目标准确率设置选项\n",
    "    \"\"\"\n",
    "    # 默认目标准确率和容差\n",
    "    target_accuracy = 0.89\n",
    "    accuracy_margin = 0.01\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\\n\" + \"=\"*60)\n",
    "        print(\"  财务造假预测模型 - 分行业逐步运行优化版  \")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\n当前目标准确率: {:.2f}% (±{:.2f}%)\".format(target_accuracy*100, accuracy_margin*100))\n",
    "        print(\"\\n请选择要执行的操作:\")\n",
    "        print(\"1. 生成可用行业列表和统计分析\")\n",
    "        print(\"2. 运行单个行业的模型\")\n",
    "        print(\"3. 运行单个行业的模型 (时间序列处理)\")\n",
    "        print(\"4. 合并所有行业的结果并生成汇总报告\")\n",
    "        print(\"5. 对未知样本进行预测\")\n",
    "        print(\"6. 对特定行业的未知样本进行预测\")\n",
    "        print(\"7. 模型性能评估与可视化\")\n",
    "        print(\"8. 数据探索性分析\")\n",
    "        print(\"9. 单个行业模型详细评估\")\n",
    "        print(\"10. 设置目标准确率和容差范围\")\n",
    "        print(\"0. 退出\")\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"\\n请输入选项 (0-10): \")\n",
    "            \n",
    "            if choice == '0':\n",
    "                logger.info(\"\\n程序结束，感谢使用！\")\n",
    "                break\n",
    "            \n",
    "            elif choice == '1':\n",
    "                # 生成行业列表\n",
    "                generate_industry_list()\n",
    "            \n",
    "            elif choice == '2':\n",
    "                # 运行单个行业模型\n",
    "                industry_list_path = None\n",
    "                for f in sorted(os.listdir('results'), reverse=True):\n",
    "                    if f.startswith('industry_list_') and f.endswith('.csv'):\n",
    "                        industry_list_path = os.path.join('results', f)\n",
    "                        break\n",
    "                \n",
    "                if industry_list_path:\n",
    "                    industry_list = pd.read_csv(industry_list_path)\n",
    "                    print(\"\\n可用行业列表:\")\n",
    "                    \n",
    "                    # 显示可用行业，并标记推荐的行业\n",
    "                    for i, row in enumerate(industry_list.iterrows()):\n",
    "                        idx, data = row\n",
    "                        industry = data['Industry']\n",
    "                        sample_count = data['Sample_Count']\n",
    "                        fraud_ratio = data['Fraud_Ratio']\n",
    "                        \n",
    "                        # 标记推荐行业\n",
    "                        is_trainable = data.get('Trainable', sample_count >= 30)\n",
    "                        is_imbalanced = data.get('Imbalanced', fraud_ratio < 0.01 or fraud_ratio > 0.99)\n",
    "                        \n",
    "                        marker = ''\n",
    "                        if is_trainable and not is_imbalanced:\n",
    "                            marker = ' (推荐)'\n",
    "                        elif not is_trainable:\n",
    "                            marker = ' (样本过少)'\n",
    "                        elif is_imbalanced:\n",
    "                            marker = ' (严重不平衡)'\n",
    "                        \n",
    "                        print(f\"{i+1}. {industry}{marker} - 样本数: {sample_count}, 造假率: {fraud_ratio:.2%}\")\n",
    "                    \n",
    "                    idx = input(\"\\n请输入行业编号，或直接输入行业名称: \")\n",
    "                    \n",
    "                    if idx.isdigit() and int(idx) <= len(industry_list):\n",
    "                        selected_industry = industry_list.iloc[int(idx)-1]['Industry']\n",
    "                    else:\n",
    "                        selected_industry = idx\n",
    "                    \n",
    "                    run_single_industry(selected_industry, time_series=False, \n",
    "                                        target_accuracy=target_accuracy, \n",
    "                                        accuracy_margin=accuracy_margin)\n",
    "                else:\n",
    "                    industry_name = input(\"\\n请输入要处理的行业名称: \")\n",
    "                    run_single_industry(industry_name, time_series=False, \n",
    "                                        target_accuracy=target_accuracy, \n",
    "                                        accuracy_margin=accuracy_margin)\n",
    "            \n",
    "            elif choice == '3':\n",
    "                # 运行单个行业模型（时间序列处理）\n",
    "                industry_name = input(\"\\n请输入要处理的行业名称: \")\n",
    "                date_col = input(\"请输入日期列名称 (默认为'report_date'): \") or \"report_date\"\n",
    "                run_single_industry(industry_name, time_series=True, date_col=date_col,\n",
    "                                   target_accuracy=target_accuracy, \n",
    "                                   accuracy_margin=accuracy_margin)\n",
    "            \n",
    "            elif choice == '4':\n",
    "                # 合并结果并生成报告\n",
    "                combine_industry_results()\n",
    "                # 在main_menu函数中的选项4中添加\n",
    "                try:\n",
    "                    combine_industry_results()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error combining industry results: {e}\", exc_info=True)\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(\"Some results may still be available.\")\n",
    "            \n",
    "            elif choice == '5':\n",
    "                # 预测未知样本\n",
    "                predict_unknown_samples()\n",
    "            \n",
    "            elif choice == '6':\n",
    "                # 预测特定行业的未知样本\n",
    "                industry_list_path = None\n",
    "                for f in sorted(os.listdir('results'), reverse=True):\n",
    "                    if f.startswith('industry_list_') and f.endswith('.csv'):\n",
    "                        industry_list_path = os.path.join('results', f)\n",
    "                        break\n",
    "                \n",
    "                if industry_list_path:\n",
    "                    industry_list = pd.read_csv(industry_list_path)\n",
    "                    print(\"\\n可用行业列表:\")\n",
    "                    for i, ind in enumerate(industry_list['Industry']):\n",
    "                        print(f\"{i+1}. {ind}\")\n",
    "                    \n",
    "                    idx = input(\"\\n请输入行业编号，或直接输入行业名称: \")\n",
    "                    \n",
    "                    if idx.isdigit() and int(idx) <= len(industry_list):\n",
    "                        selected_industry = industry_list.iloc[int(idx)-1]['Industry']\n",
    "                    else:\n",
    "                        selected_industry = idx\n",
    "                    \n",
    "                    predict_unknown_samples(selected_industry)\n",
    "                else:\n",
    "                    industry_name = input(\"\\n请输入要预测的行业名称: \")\n",
    "                    predict_unknown_samples(industry_name)\n",
    "            \n",
    "            elif choice == '7':\n",
    "                # 模型性能评估与可视化\n",
    "                # 获取所有性能报告\n",
    "                performance_files = [f for f in os.listdir('results') if f.endswith('_performance.csv')]\n",
    "                if performance_files:\n",
    "                    all_performance = []\n",
    "                    for f in performance_files:\n",
    "                        try:\n",
    "                            perf = pd.read_csv(os.path.join('results', f))\n",
    "                            all_performance.append(perf)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    if all_performance:\n",
    "                        performance_df = pd.concat(all_performance)\n",
    "                        \n",
    "                        # 创建性能对比可视化\n",
    "                        plt.figure(figsize=(15, 10))\n",
    "                        \n",
    "                        # AUC对比\n",
    "                        plt.subplot(2, 2, 1)\n",
    "                        performance_df = performance_df.sort_values('AUC', ascending=False)\n",
    "                        sns.barplot(x='AUC', y='Industry', data=performance_df)\n",
    "                        plt.title('Model Performance by AUC')\n",
    "                        plt.xlabel('AUC')\n",
    "                        plt.ylabel('Industry')\n",
    "                        plt.grid(True, axis='x')\n",
    "                        # 显示汉字\n",
    "                        plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                        plt.rcParams['axes.unicode_minus'] = False\n",
    "                        \n",
    "                        # PR-AUC对比\n",
    "                        plt.subplot(2, 2, 2)\n",
    "                        performance_df = performance_df.sort_values('PR-AUC', ascending=False)\n",
    "                        sns.barplot(x='PR-AUC', y='Industry', data=performance_df)\n",
    "                        plt.title('Model Performance by PR-AUC')\n",
    "                        plt.xlabel('PR-AUC')\n",
    "                        plt.ylabel('Industry')\n",
    "                        plt.grid(True, axis='x')\n",
    "                        \n",
    "                        # 如果包含准确率列，则显示准确率对比\n",
    "                        if 'Accuracy' in performance_df.columns:\n",
    "                            plt.subplot(2, 2, 3)\n",
    "                            performance_df = performance_df.sort_values('Accuracy', ascending=False)\n",
    "                            sns.barplot(x='Accuracy', y='Industry', data=performance_df)\n",
    "                            plt.title('Model Performance by Accuracy')\n",
    "                            plt.xlabel('Accuracy')\n",
    "                            plt.ylabel('Industry')\n",
    "                            plt.grid(True, axis='x')\n",
    "                            \n",
    "                        # 如果包含使用最佳阈值的准确率列，则显示对比\n",
    "                        if 'Accuracy_Best_Threshold' in performance_df.columns:\n",
    "                            plt.subplot(2, 2, 4)\n",
    "                            performance_df = performance_df.sort_values('Accuracy_Best_Threshold', ascending=False)\n",
    "                            sns.barplot(x='Accuracy_Best_Threshold', y='Industry', data=performance_df)\n",
    "                            plt.title('Model Performance by Accuracy (Best Threshold)')\n",
    "                            plt.xlabel('Accuracy with Best Threshold')\n",
    "                            plt.ylabel('Industry')\n",
    "                            plt.grid(True, axis='x')\n",
    "                        \n",
    "                        # 显示汉字\n",
    "                        plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                        plt.rcParams['axes.unicode_minus'] = False\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                        plt.savefig(f'plots/all_models_performance_{timestamp}.png', dpi=600, bbox_inches='tight')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        # 保存汇总报告\n",
    "                        performance_df.to_csv(f'results/all_models_performance_{timestamp}.csv', index=False)\n",
    "                        \n",
    "                        print(f\"\\n性能汇总报告已保存至 results/all_models_performance_{timestamp}.csv\")\n",
    "                        print(f\"性能对比图已保存至 plots/all_models_performance_{timestamp}.png\")\n",
    "                    else:\n",
    "                        print(\"\\n未找到性能报告。请先运行模型。\")\n",
    "                else:\n",
    "                    print(\"\\n未找到性能报告。请先运行模型。\")\n",
    "            \n",
    "            elif choice == '8':\n",
    "                # 数据探索性分析\n",
    "                print(\"\\n数据探索性分析选项:\")\n",
    "                print(\"1. 查看数据基本统计信息\")\n",
    "                print(\"2. 查看特征重要性分析\")\n",
    "                print(\"3. 查看预测结果分析\")\n",
    "                \n",
    "                analysis_choice = input(\"\\n请选择分析类型 (1-3): \")\n",
    "                \n",
    "                if analysis_choice == '1':\n",
    "                    # 基本统计信息\n",
    "                    try:\n",
    "                        financial_data = pd.read_csv('output/data_partial_balanced.csv')\n",
    "                        \n",
    "                        # 输出基本统计信息\n",
    "                        print(\"\\n数据总体信息:\")\n",
    "                        print(f\"总样本数: {len(financial_data)}\")\n",
    "                        print(f\"标记样本数: {financial_data['FLAG'].notna().sum()}\")\n",
    "                        print(f\"未标记样本数: {financial_data['FLAG'].isna().sum()}\")\n",
    "                        \n",
    "                        if 'Industry' in financial_data.columns:\n",
    "                            industry_counts = financial_data['Industry'].value_counts()\n",
    "                            print(f\"\\n行业分布 (前10):\")\n",
    "                            print(industry_counts.head(10))\n",
    "                        \n",
    "                        # 创建统计图表\n",
    "                        plt.figure(figsize=(15, 10))\n",
    "                        \n",
    "                        # 样本分布饼图\n",
    "                        plt.subplot(2, 2, 1)\n",
    "                        labeled = financial_data['FLAG'].notna().sum()\n",
    "                        unlabeled = financial_data['FLAG'].isna().sum()\n",
    "                        plt.pie([labeled, unlabeled], labels=['已标记', '未标记'], autopct='%1.1f%%')\n",
    "                        plt.title('数据标记分布')\n",
    "                        # 显示汉字\n",
    "                        plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                        plt.rcParams['axes.unicode_minus'] = False\n",
    "                        \n",
    "                        # 已标记样本中的造假比例\n",
    "                        if 'FLAG' in financial_data.columns:\n",
    "                            plt.subplot(2, 2, 2)\n",
    "                            fraud_data = financial_data[financial_data['FLAG'].notna()]\n",
    "                            fraud = fraud_data['FLAG'].sum()\n",
    "                            non_fraud = len(fraud_data) - fraud\n",
    "                            plt.pie([fraud, non_fraud], labels=['造假', '非造假'], autopct='%1.1f%%')\n",
    "                            plt.title('已标记样本中的造假比例')\n",
    "                            # 显示汉字\n",
    "                            plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                            plt.rcParams['axes.unicode_minus'] = False\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                        plt.savefig(f'plots/data_statistics_{timestamp}.png', dpi=600)\n",
    "                        plt.show()\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"分析数据时出错: {e}\")\n",
    "                \n",
    "                elif analysis_choice == '2':\n",
    "                    # 查看特征重要性\n",
    "                    feature_files = [f for f in os.listdir('results') if f.endswith('_feature_importance.csv')]\n",
    "                    \n",
    "                    if feature_files:\n",
    "                        print(\"\\n可用的特征重要性文件:\")\n",
    "                        for i, f in enumerate(feature_files):\n",
    "                            print(f\"{i+1}. {f}\")\n",
    "                        \n",
    "                        file_idx = input(\"\\n请选择要查看的文件编号: \")\n",
    "                        \n",
    "                        if file_idx.isdigit() and int(file_idx) <= len(feature_files):\n",
    "                            selected_file = feature_files[int(file_idx)-1]\n",
    "                            try:\n",
    "                                importance_df = pd.read_csv(os.path.join('results', selected_file))\n",
    "                                \n",
    "                                # 查看前N个重要特征\n",
    "                                n = input(\"要查看多少个最重要的特征? (默认为20): \") or \"20\"\n",
    "                                n = int(n)\n",
    "                                \n",
    "                                # 按模型分组显示重要特征\n",
    "                                for model in importance_df['Model'].unique():\n",
    "                                    model_data = importance_df[importance_df['Model'] == model]\n",
    "                                    model_data = model_data.sort_values('Importance', ascending=False).head(n)\n",
    "                                    \n",
    "                                    plt.figure(figsize=(10, 8))\n",
    "                                    sns.barplot(x='Importance', y='Feature', data=model_data)\n",
    "                                    plt.title(f'Top {n} Features - {model}')\n",
    "                                    # 显示汉字\n",
    "                                    plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                                    plt.rcParams['axes.unicode_minus'] = False\n",
    "                                    plt.tight_layout()\n",
    "                                    plt.show()\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"查看特征重要性时出错: {e}\")\n",
    "                    else:\n",
    "                        print(\"\\n未找到特征重要性文件。请先运行模型。\")\n",
    "                \n",
    "                elif analysis_choice == '3':\n",
    "                    # 查看预测结果分析\n",
    "                    prediction_files = [f for f in os.listdir('results') if 'predictions_' in f and f.endswith('.csv')]\n",
    "                    \n",
    "                    if prediction_files:\n",
    "                        print(\"\\n可用的预测结果文件:\")\n",
    "                        for i, f in enumerate(prediction_files):\n",
    "                            print(f\"{i+1}. {f}\")\n",
    "                        \n",
    "                        file_idx = input(\"\\n请选择要分析的文件编号: \")\n",
    "                        \n",
    "                        if file_idx.isdigit() and int(file_idx) <= len(prediction_files):\n",
    "                            selected_file = prediction_files[int(file_idx)-1]\n",
    "                            try:\n",
    "                                pred_df = pd.read_csv(os.path.join('results', selected_file))\n",
    "                                \n",
    "                                # 创建预测分析图表\n",
    "                                plt.figure(figsize=(15, 12))\n",
    "                                \n",
    "                                # 预测概率分布\n",
    "                                plt.subplot(2, 2, 1)\n",
    "                                sns.histplot(pred_df['Fraud_Probability'], bins=20)\n",
    "                                plt.title('欺诈概率分布')\n",
    "                                plt.xlabel('概率')\n",
    "                                plt.ylabel('样本数')\n",
    "                                \n",
    "                                # 按行业的欺诈率\n",
    "                                plt.subplot(2, 2, 2)\n",
    "                                industry_stats = pred_df.groupby('Industry')['Predicted_Fraud'].mean().sort_values(ascending=False)\n",
    "                                industry_stats = industry_stats.head(15)  # 显示前15个行业\n",
    "                                \n",
    "                                sns.barplot(x=industry_stats.values, y=industry_stats.index)\n",
    "                                plt.title('各行业欺诈率')\n",
    "                                plt.xlabel('欺诈率')\n",
    "                                \n",
    "                                # 样本量与欺诈率关系图\n",
    "                                plt.subplot(2, 2, 3)\n",
    "                                industry_counts = pred_df.groupby('Industry').size()\n",
    "                                industry_frauds = pred_df.groupby('Industry')['Predicted_Fraud'].mean()\n",
    "                                \n",
    "                                plt.scatter(industry_counts, industry_frauds, alpha=0.7)\n",
    "                                plt.xscale('log')\n",
    "                                plt.xlabel('样本数 (对数)')\n",
    "                                plt.ylabel('欺诈率')\n",
    "                                plt.title('样本数与欺诈率关系')\n",
    "                                # 显示汉字\n",
    "                                plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                                plt.rcParams['axes.unicode_minus'] = False\n",
    "                                \n",
    "                                # 标记一些极端点\n",
    "                                for ind in industry_frauds.index:\n",
    "                                    if industry_frauds[ind] > 0.7 or industry_frauds[ind] < 0.05 or industry_counts[ind] > 100:\n",
    "                                        plt.annotate(\n",
    "                                            ind, \n",
    "                                            (industry_counts[ind], industry_frauds[ind]),\n",
    "                                            xytext=(5, 5),\n",
    "                                            textcoords='offset points'\n",
    "                                        )\n",
    "                                        \n",
    "                                # 显示汉字\n",
    "                                plt.rcParams['font.sans-serif']=['SimHei']   \n",
    "                                plt.rcParams['axes.unicode_minus'] = False\n",
    "                                plt.tight_layout()\n",
    "                                plt.show()\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"分析预测结果时出错: {e}\")\n",
    "                    else:\n",
    "                        print(\"\\n未找到预测结果文件。请先运行预测。\")\n",
    "\n",
    "            elif choice == '9':\n",
    "                # 单个行业模型评估\n",
    "                # 创建评估目录\n",
    "                os.makedirs('evaluations', exist_ok=True)\n",
    "\n",
    "                # 获取所有可用的模型\n",
    "                model_files = []\n",
    "                for ext in ['pkl', 'joblib']:\n",
    "                    model_files.extend([f for f in os.listdir('models') if f.endswith(f'_stacking_model.{ext}')])\n",
    "\n",
    "                if not model_files:\n",
    "                    print(\"\\n未找到任何训练好的模型。请先训练模型。\")\n",
    "                    continue\n",
    "\n",
    "                # 从模型文件中提取行业名称\n",
    "                industries = []\n",
    "                for model_file in model_files:\n",
    "                    industry = model_file.split('_stacking_model.')[0]\n",
    "                    if industry not in industries:\n",
    "                        industries.append(industry)\n",
    "\n",
    "                print(\"\\n可用的行业模型:\")\n",
    "                for i, industry in enumerate(industries):\n",
    "                    print(f\"{i+1}. {industry}\")\n",
    "\n",
    "                idx = input(\"\\n请输入要评估的行业编号，或直接输入行业名称: \")\n",
    "\n",
    "                if idx.isdigit() and int(idx) <= len(industries):\n",
    "                    selected_industry = industries[int(idx)-1]\n",
    "                else:\n",
    "                    selected_industry = idx\n",
    "\n",
    "                # 评估所选行业的模型\n",
    "                result = evaluate_industry_model(selected_industry)\n",
    "                print(result)\n",
    "            \n",
    "            elif choice == '10':\n",
    "                # 设置目标准确率和容差范围\n",
    "                print(\"\\n当前目标准确率: {:.2f}% (±{:.2f}%)\".format(target_accuracy*100, accuracy_margin*100))\n",
    "                \n",
    "                try:\n",
    "                    new_target = input(\"请输入新的目标准确率 (百分比，例如 89): \")\n",
    "                    if new_target:\n",
    "                        target_accuracy = float(new_target) / 100  # 转换为小数\n",
    "                        if target_accuracy < 0 or target_accuracy > 1:\n",
    "                            print(\"目标准确率应该在0-100之间，请重新设置\")\n",
    "                            target_accuracy = 0.89  # 重置为默认值\n",
    "                    \n",
    "                    new_margin = input(\"请输入新的容差范围 (百分比，例如 1): \")\n",
    "                    if new_margin:\n",
    "                        accuracy_margin = float(new_margin) / 100  # 转换为小数\n",
    "                        if accuracy_margin < 0 or accuracy_margin > 0.1:\n",
    "                            print(\"容差范围应该在0-10之间，请重新设置\")\n",
    "                            accuracy_margin = 0.01  # 重置为默认值\n",
    "                    \n",
    "                    print(\"\\n目标准确率已更新为: {:.2f}% (±{:.2f}%)\".format(target_accuracy*100, accuracy_margin*100))\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    print(f\"输入错误: {e}\")\n",
    "                    print(\"目标准确率未更改\")\n",
    "                \n",
    "            else:\n",
    "                print(\"\\n无效选项，请重新选择\")\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行操作时出错: {e}\", exc_info=True)\n",
    "            print(f\"执行过程中出现错误: {e}\")\n",
    "            print(\"请查看日志文件了解详情。\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=== 启动财务造假预测系统 ===\")\n",
    "    main_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc871936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TICKER_SYMBOL</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Fraud_Probability</th>\n",
       "      <th>Predicted_Fraud</th>\n",
       "      <th>Threshold_Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25450</td>\n",
       "      <td>21990.0</td>\n",
       "      <td>交通运输、仓储和邮政业</td>\n",
       "      <td>0.532549</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451</td>\n",
       "      <td>43817.0</td>\n",
       "      <td>交通运输、仓储和邮政业</td>\n",
       "      <td>0.476269</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25452</td>\n",
       "      <td>362828.0</td>\n",
       "      <td>交通运输、仓储和邮政业</td>\n",
       "      <td>0.476808</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25453</td>\n",
       "      <td>403833.0</td>\n",
       "      <td>交通运输、仓储和邮政业</td>\n",
       "      <td>0.470437</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25454</td>\n",
       "      <td>453498.0</td>\n",
       "      <td>交通运输、仓储和邮政业</td>\n",
       "      <td>0.488001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823</th>\n",
       "      <td>25731</td>\n",
       "      <td>4267744.0</td>\n",
       "      <td>金融业</td>\n",
       "      <td>0.454094</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>25732</td>\n",
       "      <td>4564086.0</td>\n",
       "      <td>金融业</td>\n",
       "      <td>0.455946</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>25733</td>\n",
       "      <td>4589754.0</td>\n",
       "      <td>金融业</td>\n",
       "      <td>0.537824</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>25734</td>\n",
       "      <td>3348662.0</td>\n",
       "      <td>金融业</td>\n",
       "      <td>0.455946</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>25735</td>\n",
       "      <td>4227211.0</td>\n",
       "      <td>金融业</td>\n",
       "      <td>0.454248</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3828 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  TICKER_SYMBOL     Industry  Fraud_Probability  Predicted_Fraud  \\\n",
       "0     25450        21990.0  交通运输、仓储和邮政业           0.532549                0   \n",
       "1     25451        43817.0  交通运输、仓储和邮政业           0.476269                0   \n",
       "2     25452       362828.0  交通运输、仓储和邮政业           0.476808                0   \n",
       "3     25453       403833.0  交通运输、仓储和邮政业           0.470437                0   \n",
       "4     25454       453498.0  交通运输、仓储和邮政业           0.488001                0   \n",
       "...     ...            ...          ...                ...              ...   \n",
       "3823  25731      4267744.0          金融业           0.454094                0   \n",
       "3824  25732      4564086.0          金融业           0.455946                0   \n",
       "3825  25733      4589754.0          金融业           0.537824                0   \n",
       "3826  25734      3348662.0          金融业           0.455946                0   \n",
       "3827  25735      4227211.0          金融业           0.454248                0   \n",
       "\n",
       "      Threshold_Used  \n",
       "0               0.55  \n",
       "1               0.55  \n",
       "2               0.55  \n",
       "3               0.55  \n",
       "4               0.55  \n",
       "...              ...  \n",
       "3823            0.55  \n",
       "3824            0.55  \n",
       "3825            0.55  \n",
       "3826            0.55  \n",
       "3827            0.55  \n",
       "\n",
       "[3828 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = os.path.join('results', 'all_predictions_20250410_162227.csv')\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e402e36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各行业预测为欺诈的数量统计:\n",
      "           Industry  Fraud_Count\n",
      "8         文化、体育和娱乐业           12\n",
      "9     水利、环境和公共设施管理业            8\n",
      "12              金融业            8\n",
      "3               制造业            6\n",
      "4               建筑业            6\n",
      "0       交通运输、仓储和邮政业            4\n",
      "6            批发和零售业            4\n",
      "1   信息传输、软件和信息技术服务业            3\n",
      "2          农、林、牧、渔业            3\n",
      "5              房地产业            3\n",
      "10       科学研究和技术服务业            3\n",
      "7                教育            2\n",
      "11              采矿业            2\n"
     ]
    }
   ],
   "source": [
    "# 按照Industry分组并统计Predicted_Fraud=1的个数\n",
    "fraud_count_by_industry = df[df['Predicted_Fraud'] == 1].groupby('Industry').size()\n",
    "\n",
    "# 将结果转换为DataFrame以便更好地显示\n",
    "fraud_count_df = fraud_count_by_industry.reset_index(name='Fraud_Count')\n",
    "\n",
    "# 按欺诈数量降序排列\n",
    "fraud_count_df = fraud_count_df.sort_values(by='Fraud_Count', ascending=False)\n",
    "\n",
    "# 打印结果\n",
    "print(\"各行业预测为欺诈的数量统计:\")\n",
    "print(fraud_count_df)\n",
    "\n",
    "# # 计算总欺诈数量\n",
    "# total_fraud = df['Predicted_Fraud'].sum()\n",
    "# print(f\"\\n总欺诈预测数量: {total_fraud}\")\n",
    "# print(f\"总记录数: {len(df)}\")\n",
    "# print(f\"欺诈比例: {total_fraud/len(df):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085dc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
